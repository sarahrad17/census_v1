{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked!!\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_order = [124, 116, 118, 123, 115, 117, 125, 51, 59, 111, 32, 102, \n",
    "              108, 42, 62, 37, 99, 44, 103, 48, 26, 96, 31, 106, 95, \n",
    "              45, 105, 38, 41, 107, 119, 97, 33, 43, 30, 94, 13, 79, \n",
    "              15, 83, 80, 27, 29, 12, 88, 76, 11, 78, 4, 17, 81, 75, \n",
    "              19, 74, 14, 6, 67, 68, 1, 10, 65, 5, 64, 63, 0, 66, 69, \n",
    "              71, 70, 2, 73, 8, 7, 84, 3, 72, 91, 77, 85, 20, 82, 21,\n",
    "              24, 9, 18, 28, 104, 22, 110, 87, 25, 86, 89, 92, 35, 90,\n",
    "              40, 36, 100, 50, 47, 16, 93, 39, 23, 98, 34, 56, 114, 57, \n",
    "              101, 113, 54, 46, 52, 49, 60, 55, 120, 53, 58, 109, 112, 61, 122, 121]\n",
    "\n",
    "\n",
    "races2 = ['White alone',\n",
    "       'Black or African American alone',\n",
    "       'American Indian and Alaska Native alone',\n",
    "       'Asian alone',\n",
    "       'Native Hawaiian and Other Pacific Islander alone',\n",
    "       'Some Other Race alone',\n",
    "       'White; Black or African American',\n",
    "       'White; American Indian and Alaska Native',\n",
    "       'White; Asian',\n",
    "       'White; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native',\n",
    "       'Black or African American; Asian',\n",
    "       'Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; Some Other Race',\n",
    "       'American Indian and Alaska Native; Asian',\n",
    "       'American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'American Indian and Alaska Native; Some Other Race',\n",
    "       'Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'Asian; Some Other Race',\n",
    "       'Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native',\n",
    "       'White; Black or African American; Asian',\n",
    "       'White; Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Asian',\n",
    "       'White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; American Indian and Alaska Native; Some Other Race',\n",
    "       'White; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Asian; Some Other Race',\n",
    "       'White; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian',\n",
    "       'Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "       'Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; Asian; Some Other Race',\n",
    "       'Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "       'White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; Asian; Some Other Race',\n",
    "       'White; Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race']\n",
    "\n",
    "races = ['Total!!Population of one race!!White alone',\n",
    "           'Total!!Population of one race!!Black or African American alone',\n",
    "           'Total!!Population of one race!!American Indian and Alaska Native alone',\n",
    "           'Total!!Population of one race!!Asian alone',\n",
    "           'Total!!Population of one race!!Native Hawaiian and Other Pacific Islander alone',\n",
    "           'Total!!Population of one race!!Some Other Race alone',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Black or African American',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; American Indian and Alaska Native',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Asian',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; American Indian and Alaska Native',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; Asian',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; American Indian and Alaska Native',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; Asian',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of six races!!White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race']\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rachel's accuracy metric from page 13 of https://arxiv.org/pdf/1912.03250.pdf\n",
    "#INPUT:\n",
    "# mu: parameter\n",
    "# p_histogram: histogram (1D array of length n) of population sizes for the true data\n",
    "# q_histogram: histogram (1D array of length n) of population sizes for the de-identified data\n",
    "#--------------------------------------------------------------------------\n",
    "#OUTPUT:\n",
    "# div: a real number representing the mu-smoothed KL divergence between distributions P and Q\n",
    "def Dkl(mu, p_histogram, q_histogram):\n",
    "    div = 0\n",
    "    for i in range(0,len(p_histogram)):\n",
    "        s = (p_histogram[i] + mu)*np.log(abs((p_histogram[i] + mu)/(q_histogram[i] + mu)))\n",
    "        div += s\n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find races that are <10% in dataset\n",
    "\n",
    "def get_minor_dp(county, file, threshold, mu):\n",
    "    df_orig = pd.read_csv (r'../homemade_data/'+county+'.csv')\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('^Unnamed')]\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('SwapVal')]\n",
    "    for index, row in df_orig.iterrows():\n",
    "        d = row['race']\n",
    "        h = row['hispanic']\n",
    "        if d in races and h==1:\n",
    "            df_orig.at[index, 'race'] = races.index(d)+63\n",
    "        elif d in races and h==0:\n",
    "            df_orig.at[index, 'race'] = races.index(d)\n",
    "    labels_all = []\n",
    "    vals_all = []\n",
    "\n",
    "    #get list of occurrences by race - orig\n",
    "    for key1, value1 in df_orig.iteritems():\n",
    "        if(key1 == 'race'):\n",
    "            labs, vals = zip(*Counter(value1).items())\n",
    "            labs = list(labs)\n",
    "            vals = list(vals)\n",
    "            for i in range(0, len(real_order)):\n",
    "                if i not in labs:\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(0)\n",
    "                elif i in labs:\n",
    "                    pos = labs.index(i)\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(vals[pos])   \n",
    "    labels_all = np.arange(0,len(real_order),1)\n",
    "\n",
    "    \n",
    "    orig_nonhisp_vals = vals_all[0:63]\n",
    "    orig_hisp_vals = vals_all[63:]\n",
    "    \n",
    "    \n",
    "    orig = pd.DataFrame({'Nonhispanic':orig_nonhisp_vals, 'Hispanic':orig_hisp_vals})\n",
    "    \n",
    "    new = pd.read_csv(file)\n",
    "    new = new.loc[:, ~new.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    new_nonhisp_vals = new['Nonhispanic'].tolist()\n",
    "    new_hisp_vals = new['Hispanic'].tolist()\n",
    "    \n",
    "       \n",
    "    \n",
    "    #get percent race occurrences of each race in original data (only calculate for minorites >5%)\n",
    "    #iterate over columns to find total\n",
    "    total = 0\n",
    "    for index, row in orig.iterrows():\n",
    "        total = total + row['Hispanic']+row['Nonhispanic']\n",
    "        \n",
    "    final = new.copy()\n",
    "    for col in final.columns:\n",
    "        final[col].values[:] = 0\n",
    "        \n",
    "    orig_nonhisp_vals.extend(orig_hisp_vals)\n",
    "    new_nonhisp_vals.extend(new_hisp_vals)\n",
    "    \n",
    "    for val in orig_nonhisp_vals:\n",
    "        val = val/total\n",
    "    for val in orig_hisp_vals:\n",
    "        val = val/total        \n",
    "    for val in new_nonhisp_vals:\n",
    "        val = val/total\n",
    "    for val in new_hisp_vals:\n",
    "        val = val/total\n",
    "    \n",
    "#     print(orig_nonhisp_vals)\n",
    "#     print(new_nonhisp_vals)\n",
    "    \n",
    "    return Dkl(mu, orig_nonhisp_vals, new_nonhisp_vals)\n",
    "    \n",
    "        \n",
    "#     #find mean difference for all minority groups less than threshold\n",
    "#     for index, row in new.iterrows():\n",
    "#         if (row['Hispanic']/total <= threshold) and (row['Hispanic']-orig.at[index, 'Hispanic']>0):\n",
    "#             #CAN I DO THIS\n",
    "#             final.at[index, 'Hispanic']= abs(new.at[index,'Hispanic']-orig.at[index,'Hispanic']/(orig.at[index,'Hispanic']+1))\n",
    "#         if row['Nonhispanic']/total <= threshold and (row['Nonhispanic']-orig.at[index, 'Nonhispanic']>0):\n",
    "#             final.at[index, 'Nonhispanic']= abs(new.at[index,'Nonhispanic']-orig.at[index,'Nonhispanic']/(orig.at[index,'Nonhispanic']+1))\n",
    "    \n",
    "#     mean = (final['Nonhispanic'].sum()+final['Hispanic'].sum())/np.count_nonzero(final)\n",
    "        \n",
    "#     return mean\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alameda\n",
      "dprun_0.01_0.csv\n",
      "dprun_0.11_0.csv\n",
      "dprun_0.21_0.csv\n",
      "dprun_0.31_0.csv\n",
      "dprun_0.41_0.csv\n",
      "dprun_0.51_0.csv\n",
      "dprun_0.61_0.csv\n",
      "dprun_0.71_0.csv\n",
      "dprun_0.81_0.csv\n",
      "dprun_0.91_0.csv\n",
      "dprun_1.01_0.csv\n",
      "dprun_1.11_0.csv\n",
      "dprun_1.21_0.csv\n",
      "dprun_1.31_0.csv\n",
      "dprun_1.41_0.csv\n",
      "dprun_1.51_0.csv\n",
      "dprun_1.61_0.csv\n",
      "dprun_1.71_0.csv\n",
      "dprun_1.81_0.csv\n",
      "dprun_1.91_0.csv\n",
      "dprun_2.01_0.csv\n",
      "dprun_2.11_0.csv\n",
      "dprun_2.21_0.csv\n",
      "dprun_2.31_0.csv\n",
      "dprun_2.41_0.csv\n",
      "dprun_2.51_0.csv\n",
      "dprun_2.61_0.csv\n",
      "dprun_2.71_0.csv\n",
      "dprun_2.81_0.csv\n",
      "dprun_2.91_0.csv\n",
      "dprun_3.01_0.csv\n",
      "dprun_3.11_0.csv\n",
      "dprun_3.21_0.csv\n",
      "dprun_3.31_0.csv\n",
      "dprun_3.41_0.csv\n",
      "dprun_3.51_0.csv\n",
      "dprun_3.61_0.csv\n",
      "dprun_3.71_0.csv\n",
      "dprun_3.81_0.csv\n",
      "dprun_3.91_0.csv\n",
      "dprun_4.01_0.csv\n",
      "dprun_4.11_0.csv\n",
      "dprun_4.21_0.csv\n",
      "dprun_4.31_0.csv\n",
      "dprun_4.41_0.csv\n",
      "dprun_4.51_0.csv\n",
      "dprun_4.61_0.csv\n",
      "dprun_4.71_0.csv\n",
      "dprun_4.81_0.csv\n",
      "dprun_4.91_0.csv\n",
      "dprun_5.01_0.csv\n",
      "dprun_5.11_0.csv\n",
      "dprun_5.21_0.csv\n",
      "dprun_5.31_0.csv\n",
      "dprun_5.41_0.csv\n",
      "dprun_5.51_0.csv\n",
      "dprun_5.61_0.csv\n",
      "dprun_5.71_0.csv\n",
      "dprun_5.81_0.csv\n",
      "dprun_5.91_0.csv\n",
      "dprun_6.01_0.csv\n",
      "dprun_6.11_0.csv\n",
      "dprun_6.21_0.csv\n",
      "dprun_6.31_0.csv\n",
      "dprun_6.41_0.csv\n",
      "dprun_6.51_0.csv\n",
      "dprun_6.61_0.csv\n",
      "dprun_6.71_0.csv\n",
      "dprun_6.81_0.csv\n",
      "dprun_6.91_0.csv\n",
      "dprun_7.01_0.csv\n",
      "dprun_7.11_0.csv\n",
      "dprun_7.21_0.csv\n",
      "dprun_7.31_0.csv\n",
      "dprun_7.41_0.csv\n",
      "dprun_7.51_0.csv\n",
      "dprun_7.61_0.csv\n",
      "dprun_7.71_0.csv\n",
      "dprun_7.81_0.csv\n",
      "dprun_7.91_0.csv\n",
      "dprun_8.01_0.csv\n",
      "dprun_8.11_0.csv\n",
      "dprun_8.21_0.csv\n",
      "dprun_8.31_0.csv\n",
      "dprun_8.41_0.csv\n",
      "dprun_8.51_0.csv\n",
      "dprun_8.61_0.csv\n",
      "dprun_8.71_0.csv\n",
      "dprun_8.81_0.csv\n",
      "dprun_8.91_0.csv\n",
      "dprun_9.01_0.csv\n",
      "dprun_9.11_0.csv\n",
      "dprun_9.21_0.csv\n",
      "dprun_9.31_0.csv\n",
      "dprun_9.41_0.csv\n",
      "dprun_9.51_0.csv\n",
      "dprun_9.61_0.csv\n",
      "dprun_9.71_0.csv\n",
      "dprun_9.81_0.csv\n",
      "dprun_9.91_0.csv\n"
     ]
    }
   ],
   "source": [
    "def dp_mean_dif(county):\n",
    "    \n",
    "    ep = np.arange(.01, 10, 1, float)\n",
    "    \n",
    "    epsilon = []\n",
    "    \n",
    "    for ee in ep:\n",
    "        ee = str(round(ee,1))\n",
    "        if ee[len(ee)-1] == '.':\n",
    "            ee = str(e)+'0'\n",
    "        epsilon.append(ee)\n",
    "        \n",
    "    dfinal = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "\n",
    "    for filename in os.listdir('../dp/newdp_runs/'+county+'/'):\n",
    "            if filename.endswith(\".csv\") and '_0.csv' in filename:\n",
    "                print(filename)\n",
    "                s = 0\n",
    "                meandif = get_minor_dp(county, '../dp/newdp_runs/'+county+'/'+filename, .05, .99)\n",
    "                \n",
    "                dfinal = dfinal.append({'filename': filename, 'mean_difference': meandif}, ignore_index=True)\n",
    "                \n",
    "    sorted = {}\n",
    "    for e in epsilon:\n",
    "        #meandif, count\n",
    "        sorted[e] = []\n",
    "\n",
    "    for column_name, data in dfinal.iterrows():\n",
    "        f = data[0].rfind('_')\n",
    "        swap = data[0][0:f]\n",
    "        swap = swap[6:]\n",
    "        \n",
    "        \n",
    "        for e in epsilon:\n",
    "            if swap[:3] == e[:3]: \n",
    "                sorted[e].append(data[1])\n",
    "                break\n",
    "    \n",
    "    for e in epsilon:\n",
    "        sorted[e] = sum(sorted[e])/len(sorted[e])\n",
    "                \n",
    "    final = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "    for e in sorted:\n",
    "        final = final.append({'filename': e, 'mean_difference': sorted[e]}, ignore_index=True)\n",
    "                \n",
    "\n",
    "    fa = \"mean_dif/\"+county+\"/dp/dpnew.csv\"\n",
    "    csv_orig_data = final.to_csv(fa, index = True)\n",
    "\n",
    "    \n",
    "counties = ['Alameda', 'Armstrong', 'Cibola', 'Fayette', 'GrandForks', 'Hawaii', 'Jefferson', 'Nantucket', 'Washington']\n",
    "counties = ['alameda']\n",
    "for county in counties:\n",
    "    print(county)\n",
    "    dp_mean_dif(county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find races that are <10% in dataset\n",
    "\n",
    "def get_minor_swapping(county, file, threshold, mu):\n",
    "    df_orig = pd.read_csv (r'../homemade_data/'+county+'.csv')\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('^Unnamed')]\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('SwapVal')]\n",
    "    for index, row in df_orig.iterrows():\n",
    "        d = row['race']\n",
    "        h = row['hispanic']\n",
    "        if d in races and h==1:\n",
    "            df_orig.at[index, 'race'] = races.index(d)+63\n",
    "        elif d in races and h==0:\n",
    "            df_orig.at[index, 'race'] = races.index(d)\n",
    "    labels_all = []\n",
    "    vals_all = []\n",
    "\n",
    "    #get list of occurrences by race - orig\n",
    "    for key1, value1 in df_orig.iteritems():\n",
    "        if(key1 == 'race'):\n",
    "            labs, vals = zip(*Counter(value1).items())\n",
    "            labs = list(labs)\n",
    "            vals = list(vals)\n",
    "            for i in range(0, len(real_order)):\n",
    "                if i not in labs:\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(0)\n",
    "                elif i in labs:\n",
    "                    pos = labs.index(i)\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(vals[pos])   \n",
    "    labels_all = np.arange(0,len(real_order),1)\n",
    "    \n",
    "    orig_nonhisp_vals = vals_all[0:63]\n",
    "    orig_hisp_vals = vals_all[63:]\n",
    "    \n",
    "    orig = pd.DataFrame({'Nonhispanic':orig_nonhisp_vals, 'Hispanic':orig_hisp_vals})\n",
    "    \n",
    "       \n",
    "    #get list of occurences by race - swapping\n",
    "    df2 = pd.read_csv(file)\n",
    "    for index, row in df2.iterrows():\n",
    "        d = row['race']\n",
    "        h = row['hispanic']\n",
    "        if h==1:\n",
    "            df2.at[index, 'race'] = d+63\n",
    "    labels2_all = []\n",
    "    vals2_all = []\n",
    "    \n",
    "    for key, value in df2.iteritems():\n",
    "        if(key == 'race'):\n",
    "            labels2, values2 = zip(*Counter(value).items())\n",
    "            labels2=list(labels2)\n",
    "            values2 = list(values2)\n",
    "            for i in range(0, len(real_order)):\n",
    "                if i not in labels2:\n",
    "                    labels2_all.append(i)\n",
    "                    vals2_all.append(0)\n",
    "                elif i in labels2:\n",
    "                    pos = labels2.index(i)\n",
    "                    labels2_all.append(i)\n",
    "                    vals2_all.append(values2[pos])\n",
    "    labels2_all = np.arange(0,len(real_order),1)\n",
    "    \n",
    "    new_nonhisp_vals = vals2_all[0:63]\n",
    "    new_hisp_vals = vals2_all[63:]\n",
    "    \n",
    "    new = pd.DataFrame({'Nonhispanic':new_nonhisp_vals, 'Hispanic':new_hisp_vals})\n",
    "    \n",
    "\n",
    "    #get percent race occurrences of each race in original data (only calculate for minorites >5%)\n",
    "    #iterate over columns to find total\n",
    "    total = 0\n",
    "    for index, row in orig.iterrows():\n",
    "        total = total + row['Hispanic']+row['Nonhispanic']\n",
    "        \n",
    "    final = orig.copy()\n",
    "    for col in final.columns:\n",
    "        final[col].values[:] = 0\n",
    "       \n",
    "    orig_nonhisp_vals.extend(orig_hisp_vals)\n",
    "    new_nonhisp_vals.extend(new_hisp_vals)\n",
    "        \n",
    "    return Dkl(mu, orig_nonhisp_vals, new_nonhisp_vals)\n",
    "    \n",
    "#     #find mean difference for all minority groups less than threshold\n",
    "#     for index, row in new.iterrows():\n",
    "#         if row['Hispanic']/total <= threshold and (row['Hispanic']-orig.at[index, 'Hispanic'] >0):\n",
    "#             #CAN I DO THIS\n",
    "#             final.at[index, 'Hispanic']= abs(new.at[index,'Hispanic']-orig.at[index,'Hispanic']/(orig.at[index,'Hispanic']+1))\n",
    "#         if row['Nonhispanic']/total <= threshold and (row['Nonhispanic']-orig.at[index, 'Nonhispanic']>0):\n",
    "#             final.at[index, 'Nonhispanic']= abs(new.at[index,'Nonhispanic']-orig.at[index,'Nonhispanic']/(orig.at[index,'Nonhispanic']+1))\n",
    "    \n",
    "#     mean = (final['Nonhispanic'].sum()+final['Hispanic'].sum())/np.count_nonzero(final)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alameda\n",
      "swap_0.01_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.060000000000000005_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.11_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.16000000000000003_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.21000000000000002_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.26_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.31000000000000005_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.36000000000000004_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.41000000000000003_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.46_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.51_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.56_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.6100000000000001_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.66_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.7100000000000001_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.76_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.81_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.8600000000000001_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.91_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "swap_0.9600000000000001_a0.csv\n",
      "[687, 257, 4, 114, 0, 1, 11, 11, 20, 1, 6, 4, 7, 0, 0, 1, 0, 1, 1, 3, 0, 8, 4, 0, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 194, 7, 10, 1, 0, 42, 4, 4, 8, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def similar_mean_dif(county):\n",
    "    dfinal = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "\n",
    "    for filename in os.listdir('../swapping/swap_runs3/'+county+'/similar/'):\n",
    "            if filename.endswith(\".csv\") and '_a0.csv' in filename:\n",
    "                print(filename)\n",
    "                meandif = get_minor_swapping(county, '../swapping/swap_runs3/'+county+'/similar/'+filename, .05, 1)\n",
    "                \n",
    "                dfinal = dfinal.append({'filename': filename, 'mean_difference': meandif}, ignore_index=True)\n",
    "                \n",
    "    swaprates = np.arange(.01, .5, .05, float)\n",
    "    \n",
    "    sorted = {}\n",
    "    for s in swaprates:\n",
    "        #meandif, count\n",
    "        sorted[s] = []\n",
    "   \n",
    "    for column_name, data in dfinal.iterrows():\n",
    "        f = data[0].rfind('_')\n",
    "        swap = data[0][0:f]\n",
    "        swap = swap[5:]\n",
    "        \n",
    "        for s in swaprates:\n",
    "            spos = str(s)[0:f]\n",
    "            if swap == spos:           \n",
    "                sorted[s].append(data[1])\n",
    "                break\n",
    "    \n",
    "    for s in swaprates:\n",
    "        sorted[s] = sum(sorted[s])/len(sorted[s])\n",
    "                \n",
    "    final = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "    for s in sorted:\n",
    "        final = final.append({'filename': s, 'mean_difference': sorted[s]}, ignore_index=True)\n",
    "                \n",
    "        \n",
    "    \n",
    "    fa = \"mean_dif/\"+county+\"/similar/newswapping.csv\"\n",
    "    csv_orig_data = final.to_csv(fa, index = True)\n",
    "\n",
    "    \n",
    "counties = ['Alameda', 'Armstrong', 'Cibola', 'Fayette', 'GrandForks', 'Hawaii', 'Jefferson', 'Nantucket', 'Washington']\n",
    "counties = ['alameda']\n",
    "for county in counties:\n",
    "    print(county)\n",
    "    similar_mean_dif(county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
