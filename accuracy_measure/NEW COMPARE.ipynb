{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked!!\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_order = [124, 116, 118, 123, 115, 117, 125, 51, 59, 111, 32, 102, \n",
    "              108, 42, 62, 37, 99, 44, 103, 48, 26, 96, 31, 106, 95, \n",
    "              45, 105, 38, 41, 107, 119, 97, 33, 43, 30, 94, 13, 79, \n",
    "              15, 83, 80, 27, 29, 12, 88, 76, 11, 78, 4, 17, 81, 75, \n",
    "              19, 74, 14, 6, 67, 68, 1, 10, 65, 5, 64, 63, 0, 66, 69, \n",
    "              71, 70, 2, 73, 8, 7, 84, 3, 72, 91, 77, 85, 20, 82, 21,\n",
    "              24, 9, 18, 28, 104, 22, 110, 87, 25, 86, 89, 92, 35, 90,\n",
    "              40, 36, 100, 50, 47, 16, 93, 39, 23, 98, 34, 56, 114, 57, \n",
    "              101, 113, 54, 46, 52, 49, 60, 55, 120, 53, 58, 109, 112, 61, 122, 121]\n",
    "\n",
    "\n",
    "races2 = ['White alone',\n",
    "       'Black or African American alone',\n",
    "       'American Indian and Alaska Native alone',\n",
    "       'Asian alone',\n",
    "       'Native Hawaiian and Other Pacific Islander alone',\n",
    "       'Some Other Race alone',\n",
    "       'White; Black or African American',\n",
    "       'White; American Indian and Alaska Native',\n",
    "       'White; Asian',\n",
    "       'White; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native',\n",
    "       'Black or African American; Asian',\n",
    "       'Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; Some Other Race',\n",
    "       'American Indian and Alaska Native; Asian',\n",
    "       'American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'American Indian and Alaska Native; Some Other Race',\n",
    "       'Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'Asian; Some Other Race',\n",
    "       'Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native',\n",
    "       'White; Black or African American; Asian',\n",
    "       'White; Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Asian',\n",
    "       'White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; American Indian and Alaska Native; Some Other Race',\n",
    "       'White; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Asian; Some Other Race',\n",
    "       'White; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian',\n",
    "       'Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "       'Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; Asian; Some Other Race',\n",
    "       'Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "       'White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; Asian; Some Other Race',\n",
    "       'White; Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race']\n",
    "\n",
    "races = ['Total!!Population of one race!!White alone',\n",
    "           'Total!!Population of one race!!Black or African American alone',\n",
    "           'Total!!Population of one race!!American Indian and Alaska Native alone',\n",
    "           'Total!!Population of one race!!Asian alone',\n",
    "           'Total!!Population of one race!!Native Hawaiian and Other Pacific Islander alone',\n",
    "           'Total!!Population of one race!!Some Other Race alone',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Black or African American',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; American Indian and Alaska Native',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Asian',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; American Indian and Alaska Native',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; Asian',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; American Indian and Alaska Native',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; Asian',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of six races!!White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race']\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metric from page 13 of https://arxiv.org/pdf/1912.03250.pdf\n",
    "\n",
    "#INPUT:\n",
    "# mu: parameter\n",
    "# p_histogram: histogram (1D array of length n) of population sizes for the true data\n",
    "# q_histogram: histogram (1D array of length n) of population sizes for the de-identified data\n",
    "#--------------------------------------------------------------------------\n",
    "#OUTPUT:\n",
    "# div: a real number representing the mu-smoothed KL divergence between distributions P and Q\n",
    "def Dkl(mu, p_histogram, q_histogram):\n",
    "    div = 0\n",
    "    for i in range(len(p_histogram)):\n",
    "        s = (p_histogram[i] + mu)*np.log((p_histogram[i] + mu)/(q_histogram[i] + mu))\n",
    "        div += s\n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns mu-smoothed KL divergence between a given original dataset & a de-identified dataset \n",
    "\n",
    "#INPUT:\n",
    "# county: string of the county from which to use original data\n",
    "#file: swapping file to evaluate performance of \n",
    "#threshold: cut off for consideration as minority group \n",
    "#mu: parameter for Dkl\n",
    "#--------------------------------------------------------------------------\n",
    "#OUTPUT:\n",
    "#Returns real-valued mu-smoothed KL divergence value \n",
    "\n",
    "def get_minor_swapping(county, file, threshold, mu):\n",
    "    #import original dataset & delete unnecessary rows\n",
    "    df_orig = pd.read_csv (r'../homemade_data/'+county+'.csv')\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('^Unnamed')]\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('SwapVal')]\n",
    "    #replace race w/ value 0-126; where 0-62 represent non-hispanic and 63-126 represent hispanic\n",
    "    for index, row in df_orig.iterrows():\n",
    "        d = row['race']\n",
    "        h = row['hispanic']\n",
    "        #if hispanic, increment to 63-126 range\n",
    "        if d in races and h==1:\n",
    "            df_orig.at[index, 'race'] = races.index(d)+63\n",
    "        #else, 0-63 range\n",
    "        elif d in races and h==0:\n",
    "            df_orig.at[index, 'race'] = races.index(d)\n",
    "            \n",
    "\n",
    "    #get list of occurrences by race using counter - original dataset\n",
    "    #LABELS_ALL = race numbers 0-126\n",
    "    #VALUES_ALL = associated counts for race numbers 0-126\n",
    "    labels_all = []\n",
    "    vals_all = []\n",
    "    for key1, value1 in df_orig.iteritems():\n",
    "        if(key1 == 'race'):\n",
    "            labs, vals = zip(*Counter(value1).items())\n",
    "            labs = list(labs)\n",
    "            vals = list(vals)\n",
    "            #for each race value, get number of occurences and append label and value of each\n",
    "            for i in range(0, len(real_order)):\n",
    "                if i not in labs:\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(0)\n",
    "                elif i in labs:\n",
    "                    pos = labs.index(i)\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(vals[pos])   \n",
    "    labels_all = np.arange(0,len(real_order),1)\n",
    "    \n",
    "    #get associated race counts for hispanic and non-hispanic\n",
    "    orig_nonhisp_vals = vals_all[0:63]\n",
    "    orig_hisp_vals = vals_all[63:]\n",
    "    \n",
    "    #create dataframe of his/non-his counts for each race; for ORIGINAL dataframe\n",
    "    orig = pd.DataFrame({'Nonhispanic':orig_nonhisp_vals, 'Hispanic':orig_hisp_vals})\n",
    "           \n",
    "    #import original dataset & delete unnecessary rows\n",
    "    df2 = pd.read_csv(file)\n",
    "    for index, row in df2.iterrows():\n",
    "        d = row['race']\n",
    "        h = row['hispanic']\n",
    "        #if hispanic, increment to 63-126 range\n",
    "        #0-62 represent non-hispanic and 63-126 represent hispanic\n",
    "        if h==1:\n",
    "            df2.at[index, 'race'] = d+63\n",
    "    \n",
    "    #get list of occurrences by race using counter - de-identified swapping dataset\n",
    "    #LABELS2_ALL = race numbers 0-126\n",
    "    #VALUES2_ALL = associated counts for race numbers 0-126\n",
    "    labels2_all = []\n",
    "    vals2_all = []\n",
    "    for key, value in df2.iteritems():\n",
    "        if(key == 'race'):\n",
    "            labels2, values2 = zip(*Counter(value).items())\n",
    "            labels2=list(labels2)\n",
    "            values2 = list(values2)\n",
    "            for i in range(0, len(real_order)):\n",
    "                if i not in labels2:\n",
    "                    labels2_all.append(i)\n",
    "                    vals2_all.append(0)\n",
    "                elif i in labels2:\n",
    "                    pos = labels2.index(i)\n",
    "                    labels2_all.append(i)\n",
    "                    vals2_all.append(values2[pos])\n",
    "    labels2_all = np.arange(0,len(real_order),1)\n",
    "    \n",
    "    #get associated race counts for hispanic and non-hispanic\n",
    "    new_nonhisp_vals = vals2_all[0:63]\n",
    "    new_hisp_vals = vals2_all[63:]\n",
    "    \n",
    "    #create dataframe of his/non-his counts for each race; for ORIGINAL dataframe\n",
    "    new = pd.DataFrame({'Nonhispanic':new_nonhisp_vals, 'Hispanic':new_hisp_vals})\n",
    "    \n",
    "\n",
    "    \n",
    "    #iterate over columns to find total\n",
    "    total = 0\n",
    "    total_nonhisp = 0\n",
    "    for index, row in orig.iterrows():\n",
    "        total = total + row['Hispanic']+row['Nonhispanic']\n",
    "        total_nonhisp += row['Nonhispanic']\n",
    "        \n",
    "    final = orig.copy()\n",
    "    for col in final.columns:\n",
    "        final[col].values[:] = 0\n",
    "       \n",
    "    \n",
    "    \n",
    "    orig_nonhisp_vals.extend(orig_hisp_vals)\n",
    "    new_nonhisp_vals.extend(new_hisp_vals)\n",
    "\n",
    "    return Dkl(mu, orig_nonhisp_vals, new_nonhisp_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_mean_dif(county):\n",
    "    dfinal = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "\n",
    "    for filename in os.listdir('../dp/newdp_runs'+county):\n",
    "            if filename.endswith(\".csv\") and '_0.csv' in filename:\n",
    "                print(filename)\n",
    "                meandif = get_minor_swapping(county, '../dp/newdp_runs/'+county+filename, .05, 1)\n",
    "                \n",
    "                dfinal = dfinal.append({'filename': filename, 'mean_difference': meandif}, ignore_index=True)\n",
    "                \n",
    "    swaprates = np.arange(.01, .5, .05, float)\n",
    "    \n",
    "    sorted = {}\n",
    "    for s in swaprates:\n",
    "        #meandif, count\n",
    "        sorted[s] = []\n",
    "   \n",
    "    for column_name, data in dfinal.iterrows():\n",
    "        f = data[0].rfind('_')\n",
    "        swap = data[0][0:f]\n",
    "        swap = swap[5:]\n",
    "        \n",
    "        for s in swaprates:\n",
    "            spos = str(s)[0:f]\n",
    "            if swap == spos:           \n",
    "                sorted[s].append(data[1])\n",
    "                break\n",
    "    \n",
    "    for s in swaprates:\n",
    "        sorted[s] = sum(sorted[s])/len(sorted[s])\n",
    "                \n",
    "    final = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "    for s in sorted:\n",
    "        final = final.append({'filename': s, 'mean_difference': sorted[s]}, ignore_index=True)\n",
    "                \n",
    "        \n",
    "    \n",
    "    fa = \"mean_dif/\"+county+\"/similar/newswapping.csv\"\n",
    "    csv_orig_data = final.to_csv(fa, index = True)\n",
    "\n",
    "    \n",
    "counties = ['Alameda', 'Armstrong', 'Cibola', 'Fayette', 'GrandForks', 'Hawaii', 'Jefferson', 'Nantucket', 'Washington']\n",
    "counties = ['alameda']\n",
    "for county in counties:\n",
    "    print(county)\n",
    "    similar_mean_dif(county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_minor_dp(county, file, threshold, mu):\n",
    "    df_orig = pd.read_csv (r'../homemade_data/'+county+'.csv')\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('^Unnamed')]\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('SwapVal')]\n",
    "    for index, row in df_orig.iterrows():\n",
    "        d = row['race']\n",
    "        h = row['hispanic']\n",
    "        if d in races and h==1:\n",
    "            df_orig.at[index, 'race'] = races.index(d)+63\n",
    "        elif d in races and h==0:\n",
    "            df_orig.at[index, 'race'] = races.index(d)\n",
    "    labels_all = []\n",
    "    vals_all = []\n",
    "\n",
    "    #get list of occurrences by race - orig\n",
    "    for key1, value1 in df_orig.iteritems():\n",
    "        if(key1 == 'race'):\n",
    "            labs, vals = zip(*Counter(value1).items())\n",
    "            labs = list(labs)\n",
    "            vals = list(vals)\n",
    "            for i in range(0, len(real_order)):\n",
    "                if i not in labs:\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(0)\n",
    "                elif i in labs:\n",
    "                    pos = labs.index(i)\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(vals[pos])   \n",
    "    labels_all = np.arange(0,len(real_order),1)\n",
    "    \n",
    "    orig_nonhisp_vals = vals_all[0:63]\n",
    "    orig_hisp_vals = vals_all[63:]\n",
    "    \n",
    "    orig = pd.DataFrame({'Nonhispanic':orig_nonhisp_vals, 'Hispanic':orig_hisp_vals})\n",
    "    \n",
    "       \n",
    "    #get list of occurences by race - swapping\n",
    "    q_histogram = [0 for i in range(126)]\n",
    "    p_histogram = [0 for i in range(126)]\n",
    "    df2 = pd.read_csv(file)\n",
    "#     for index, row in df2.iterrows():\n",
    "#         q_histogram[index] = row['Nonhispanic']\n",
    "#         q_histogram[index+63] = row['Hispanic']\n",
    "#     q_histogram = np.array(q_histogram)\n",
    "    labels2_all = []\n",
    "    vals2_all = []\n",
    "    \n",
    "    for key, value in df2.iteritems():\n",
    "        if(key == 'race'):\n",
    "            labels2, values2 = zip(*Counter(value).items())\n",
    "            labels2=list(labels2)\n",
    "            values2 = list(values2)\n",
    "            for i in range(0, len(real_order)):\n",
    "                if i not in labels2:\n",
    "                    labels2_all.append(i)\n",
    "                    vals2_all.append(0)\n",
    "                elif i in labels2:\n",
    "                    pos = labels2.index(i)\n",
    "                    labels2_all.append(i)\n",
    "                    vals2_all.append(values2[pos])\n",
    "    labels2_all = np.arange(0,len(real_order),1)\n",
    "    \n",
    "    new_nonhisp_vals = vals2_all[0:63]\n",
    "    new_hisp_vals = vals2_all[63:]\n",
    "    \n",
    "    new = pd.DataFrame({'Nonhispanic':new_nonhisp_vals, 'Hispanic':new_hisp_vals})\n",
    "    \n",
    "\n",
    "    #get percent race occurrences of each race in original data (only calculate for minorites >5%)\n",
    "    #iterate over columns to find total\n",
    "    total = 0\n",
    "    total_nonhisp = 0\n",
    "    for index, row in orig.iterrows():\n",
    "        total = total + row['Hispanic']+row['Nonhispanic']\n",
    "        total_nonhisp += row['Nonhispanic']\n",
    "        \n",
    "    final = orig.copy()\n",
    "    for col in final.columns:\n",
    "        final[col].values[:] = 0\n",
    "        \n",
    "    orig_nonhisp_vals.extend(orig_hisp_vals)\n",
    "    new_nonhisp_vals.extend(new_hisp_vals)\n",
    "        \n",
    "        \n",
    "        #find mean difference for all minority groups less than threshold\n",
    "    for index, val in enumerate(orig_nonhisp_vals):\n",
    "        if val/total <= threshold and index <= 62:\n",
    "            #CAN I DO THIS\n",
    "            q_histogram[index] = df2.at[index, 'Nonhispanic']\n",
    "            p_histogram[index] = val\n",
    "        if val/total <= threshold and index >= 63:\n",
    "            q_histogram[index] = df2.at[index-63, 'Hispanic']\n",
    "            p_histogram[index] = val\n",
    "    q_histogram = np.array(q_histogram)\n",
    "    p_histogram = np.array(p_histogram)\n",
    "            \n",
    "       \n",
    "\n",
    "    q_histogram_norm = np.abs(q_histogram)\n",
    "    orig_nonhisp_vals = np.array(orig_nonhisp_vals)\n",
    "#     print(p_histogram/np.sum(p_histogram))\n",
    "#     print(q_histogram_norm/np.sum(q_histogram_norm))\n",
    "#     print(orig_nonhisp_vals - q_histogram_norm)\n",
    "#     return Dkl(mu, orig_nonhisp_vals/total, q_histogram_norm/np.sum(q_histogram_norm))\n",
    "    return Dkl(mu, p_histogram/np.sum(p_histogram), q_histogram_norm/np.sum(q_histogram_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon: 0.51         div: 0.6360151843127203\n",
      "epsilon: 1.01         div: 0.4423505542286938\n",
      "epsilon: 2.01         div: 0.15719113640184904\n",
      "epsilon: 3.01         div: 0.08864317892952572\n",
      "epsilon: 4.01         div: 0.01073872046281095\n",
      "epsilon: 5.01         div: 0.024287504347231568\n",
      "epsilon: 6.01         div: 0.003074277378765848\n",
      "epsilon: 7.01         div: 0.0\n",
      "epsilon: 8.01         div: 0.003074277378765848\n",
      "epsilon: 9.01         div: 0.0\n"
     ]
    }
   ],
   "source": [
    "county = 'alameda'\n",
    "xs = np.arange(0, 10, .1)\n",
    "ys = []\n",
    "eps = ['0.51', '1.01', '2.01', '3.01', '4.01', '5.01', '6.01', '7.01', '8.01', '9.01']\n",
    "# for filename in os.listdir('../dp/newdp_runs/'+county):\n",
    "#     if filename.endswith(\".csv\") and '_0.csv' in filename:\n",
    "#         print(filename)\n",
    "#         div = get_minor_dp(county, '../dp/newdp_runs/'+county+'/' +filename, 1, .1)\n",
    "        \n",
    "#         print(div)\n",
    "#         ys.append(div)\n",
    "\n",
    "# eps = ['100', '200', '500', '1000']\n",
    "for ep in eps:\n",
    "    div = get_minor_dp(county, '../dp/newdp_runs/'+county+'/dprun_' + ep + '_0.csv', .1, .0001)\n",
    "    print('epsilon: ' + ep + '         ' + 'div: ' + str(div))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington\n",
      "dprun_0.01_0.csv\n",
      "dprun_0.11_0.csv\n",
      "dprun_0.21_0.csv\n",
      "dprun_0.31_0.csv\n",
      "dprun_0.41_0.csv\n",
      "dprun_0.51_0.csv\n",
      "dprun_0.61_0.csv\n",
      "dprun_0.71_0.csv\n",
      "dprun_0.81_0.csv\n",
      "dprun_0.91_0.csv\n",
      "dprun_1.01_0.csv\n",
      "dprun_1.11_0.csv\n",
      "dprun_1.21_0.csv\n",
      "dprun_1.31_0.csv\n",
      "dprun_1.41_0.csv\n",
      "dprun_1.51_0.csv\n",
      "dprun_1.61_0.csv\n",
      "dprun_1.71_0.csv\n",
      "dprun_1.81_0.csv\n",
      "dprun_1.91_0.csv\n",
      "dprun_2.01_0.csv\n",
      "dprun_2.11_0.csv\n",
      "dprun_2.21_0.csv\n",
      "dprun_2.31_0.csv\n",
      "dprun_2.41_0.csv\n",
      "dprun_2.51_0.csv\n",
      "dprun_2.61_0.csv\n",
      "dprun_2.71_0.csv\n",
      "dprun_2.81_0.csv\n",
      "dprun_2.91_0.csv\n",
      "dprun_3.01_0.csv\n",
      "dprun_3.11_0.csv\n",
      "dprun_3.21_0.csv\n",
      "dprun_3.31_0.csv\n",
      "dprun_3.41_0.csv\n",
      "dprun_3.51_0.csv\n",
      "dprun_3.61_0.csv\n",
      "dprun_3.71_0.csv\n",
      "dprun_3.81_0.csv\n",
      "dprun_3.91_0.csv\n",
      "dprun_4.01_0.csv\n",
      "dprun_4.11_0.csv\n",
      "dprun_4.21_0.csv\n",
      "dprun_4.31_0.csv\n",
      "dprun_4.41_0.csv\n",
      "dprun_4.51_0.csv\n",
      "dprun_4.61_0.csv\n",
      "dprun_4.71_0.csv\n",
      "dprun_4.81_0.csv\n",
      "dprun_4.91_0.csv\n",
      "dprun_5.01_0.csv\n",
      "dprun_5.11_0.csv\n",
      "dprun_5.21_0.csv\n",
      "dprun_5.31_0.csv\n",
      "dprun_5.41_0.csv\n",
      "dprun_5.51_0.csv\n",
      "dprun_5.61_0.csv\n",
      "dprun_5.71_0.csv\n",
      "dprun_5.81_0.csv\n",
      "dprun_5.91_0.csv\n",
      "dprun_6.01_0.csv\n",
      "dprun_6.11_0.csv\n",
      "dprun_6.21_0.csv\n",
      "dprun_6.31_0.csv\n",
      "dprun_6.41_0.csv\n",
      "dprun_6.51_0.csv\n",
      "dprun_6.61_0.csv\n",
      "dprun_6.71_0.csv\n",
      "dprun_6.81_0.csv\n",
      "dprun_6.91_0.csv\n",
      "dprun_7.01_0.csv\n",
      "dprun_7.11_0.csv\n",
      "dprun_7.21_0.csv\n",
      "dprun_7.31_0.csv\n",
      "dprun_7.41_0.csv\n",
      "dprun_7.51_0.csv\n",
      "dprun_7.61_0.csv\n",
      "dprun_7.71_0.csv\n",
      "dprun_7.81_0.csv\n",
      "dprun_7.91_0.csv\n",
      "dprun_8.01_0.csv\n",
      "dprun_8.11_0.csv\n",
      "dprun_8.21_0.csv\n",
      "dprun_8.31_0.csv\n",
      "dprun_8.41_0.csv\n",
      "dprun_8.51_0.csv\n",
      "dprun_8.61_0.csv\n",
      "dprun_8.71_0.csv\n",
      "dprun_8.81_0.csv\n",
      "dprun_8.91_0.csv\n",
      "dprun_9.01_0.csv\n",
      "dprun_9.11_0.csv\n",
      "dprun_9.21_0.csv\n",
      "dprun_9.31_0.csv\n",
      "dprun_9.41_0.csv\n",
      "dprun_9.51_0.csv\n",
      "dprun_9.61_0.csv\n",
      "dprun_9.71_0.csv\n",
      "dprun_9.81_0.csv\n",
      "dprun_9.91_0.csv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5132159ca171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcounty\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcounties\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mdp_mean_dif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-5132159ca171>\u001b[0m in \u001b[0;36mdp_mean_dif\u001b[1;34m(county)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0msorted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filename'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mean_difference'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "def dp_mean_dif(county):\n",
    "    \n",
    "    ep = np.arange(.01, 10, .05, float)\n",
    "    \n",
    "    epsilon = []\n",
    "    \n",
    "    for ee in ep:\n",
    "        ee = str(round(ee,1))\n",
    "        if ee[len(ee)-1] == '.':\n",
    "            ee = str(e)+'0'\n",
    "        epsilon.append(ee)\n",
    "        \n",
    "    dfinal = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "\n",
    "    for filename in os.listdir('../dp/newdp_runs2/'+county+'/'):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                if '_0.csv' in filename:\n",
    "                    print(filename)\n",
    "                s = 0\n",
    "                meandif = get_minor_dp(county, '../dp/newdp_runs2/'+county+'/'+filename, .1, .0001)\n",
    "                \n",
    "                dfinal = dfinal.append({'filename': filename, 'mean_difference': meandif}, ignore_index=True)\n",
    "                \n",
    "    sorted = {}\n",
    "    for e in epsilon:\n",
    "        #meandif, count\n",
    "        sorted[e] = []\n",
    "\n",
    "    for column_name, data in dfinal.iterrows():\n",
    "        f = data[0].rfind('_')\n",
    "        swap = data[0][0:f]\n",
    "        swap = swap[6:]\n",
    "        \n",
    "        \n",
    "        for e in epsilon:\n",
    "            if swap[:3] == e[:3]: \n",
    "                sorted[e].append(data[1])\n",
    "                break\n",
    "    \n",
    "    for e in epsilon:\n",
    "        sorted[e] = sum(sorted[e])/len(sorted[e])\n",
    "                \n",
    "    final = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "    for e in sorted:\n",
    "        final = final.append({'filename': e, 'mean_difference': sorted[e]}, ignore_index=True)\n",
    "                \n",
    "\n",
    "    fa = \"mean_dif/\"+county+\"/dp/dpnew.csv\"\n",
    "    csv_orig_data = final.to_csv(fa, index = True)\n",
    "\n",
    "    \n",
    "counties = ['Alameda', 'Armstrong', 'Cibola', 'Fayette', 'GrandForks', 'Hawaii', 'Jefferson', 'Nantucket', 'Washington']\n",
    "counties = ['Alameda', 'Armstrong', 'Cibola', 'Fayette', 'GrandForks', 'Hawaii', 'Jefferson', 'Nantucket']\n",
    "counties = ['Washington']\n",
    "for county in counties:\n",
    "    print(county)\n",
    "    dp_mean_dif(county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
