{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked!!\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_order = [124, 116, 118, 123, 115, 117, 125, 51, 59, 111, 32, 102, \n",
    "              108, 42, 62, 37, 99, 44, 103, 48, 26, 96, 31, 106, 95, \n",
    "              45, 105, 38, 41, 107, 119, 97, 33, 43, 30, 94, 13, 79, \n",
    "              15, 83, 80, 27, 29, 12, 88, 76, 11, 78, 4, 17, 81, 75, \n",
    "              19, 74, 14, 6, 67, 68, 1, 10, 65, 5, 64, 63, 0, 66, 69, \n",
    "              71, 70, 2, 73, 8, 7, 84, 3, 72, 91, 77, 85, 20, 82, 21,\n",
    "              24, 9, 18, 28, 104, 22, 110, 87, 25, 86, 89, 92, 35, 90,\n",
    "              40, 36, 100, 50, 47, 16, 93, 39, 23, 98, 34, 56, 114, 57, \n",
    "              101, 113, 54, 46, 52, 49, 60, 55, 120, 53, 58, 109, 112, 61, 122, 121]\n",
    "\n",
    "\n",
    "races2 = ['White alone',\n",
    "       'Black or African American alone',\n",
    "       'American Indian and Alaska Native alone',\n",
    "       'Asian alone',\n",
    "       'Native Hawaiian and Other Pacific Islander alone',\n",
    "       'Some Other Race alone',\n",
    "       'White; Black or African American',\n",
    "       'White; American Indian and Alaska Native',\n",
    "       'White; Asian',\n",
    "       'White; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native',\n",
    "       'Black or African American; Asian',\n",
    "       'Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; Some Other Race',\n",
    "       'American Indian and Alaska Native; Asian',\n",
    "       'American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'American Indian and Alaska Native; Some Other Race',\n",
    "       'Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'Asian; Some Other Race',\n",
    "       'Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native',\n",
    "       'White; Black or African American; Asian',\n",
    "       'White; Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Asian',\n",
    "       'White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; American Indian and Alaska Native; Some Other Race',\n",
    "       'White; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Asian; Some Other Race',\n",
    "       'White; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian',\n",
    "       'Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "       'Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; Asian; Some Other Race',\n",
    "       'Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "       'White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; Asian; Some Other Race',\n",
    "       'White; Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "       'White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race']\n",
    "\n",
    "races = ['Total!!Population of one race!!White alone',\n",
    "           'Total!!Population of one race!!Black or African American alone',\n",
    "           'Total!!Population of one race!!American Indian and Alaska Native alone',\n",
    "           'Total!!Population of one race!!Asian alone',\n",
    "           'Total!!Population of one race!!Native Hawaiian and Other Pacific Islander alone',\n",
    "           'Total!!Population of one race!!Some Other Race alone',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Black or African American',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; American Indian and Alaska Native',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Asian',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!White; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; American Indian and Alaska Native',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; Asian',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!Black or African American; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of two races!!Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of two races!!Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; American Indian and Alaska Native',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; Asian',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Black or African American; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!White; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of three races!!Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Asian',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!White; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of four races!!American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of five races!!Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "           'Total!!Two or More Races!!Population of six races!!White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race']\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rachel's accuracy metric from page 13 of https://arxiv.org/pdf/1912.03250.pdf\n",
    "#INPUT:\n",
    "# mu: parameter\n",
    "# p_histogram: histogram (1D array of length n) of population sizes for the true data\n",
    "# q_histogram: histogram (1D array of length n) of population sizes for the de-identified data\n",
    "#--------------------------------------------------------------------------\n",
    "#OUTPUT:\n",
    "# div: a real number representing the mu-smoothed KL divergence between distributions P and Q\n",
    "def Dkl(mu, p_histogram, q_histogram):\n",
    "    div = 0\n",
    "#     print(p_histogram)\n",
    "    for i in range(len(p_histogram)):\n",
    "        s = (p_histogram[i] + mu)*np.log((p_histogram[i] + mu)/(q_histogram[i] + mu))\n",
    "        div += s\n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find races that are <10% in dataset\n",
    "\n",
    "def get_minor_dp(county, file, threshold):\n",
    "    df_orig = pd.read_csv (r'../homemade_data/'+county+'.csv')\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('^Unnamed')]\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('SwapVal')]\n",
    "    for index, row in df_orig.iterrows():\n",
    "        d = row['race']\n",
    "        h = row['hispanic']\n",
    "        if d in races and h==1:\n",
    "            df_orig.at[index, 'race'] = races.index(d)+63\n",
    "        elif d in races and h==0:\n",
    "            df_orig.at[index, 'race'] = races.index(d)\n",
    "    labels_all = []\n",
    "    vals_all = []\n",
    "\n",
    "    #get list of occurrences by race - orig\n",
    "    for key1, value1 in df_orig.iteritems():\n",
    "        if(key1 == 'race'):\n",
    "            labs, vals = zip(*Counter(value1).items())\n",
    "            labs = list(labs)\n",
    "            vals = list(vals)\n",
    "            for i in range(0, len(real_order)):\n",
    "                if i not in labs:\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(0)\n",
    "                elif i in labs:\n",
    "                    pos = labs.index(i)\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(vals[pos])   \n",
    "    labels_all = np.arange(0,len(real_order),1)\n",
    "\n",
    "    \n",
    "    orig_nonhisp_vals = vals_all[0:63]\n",
    "    orig_hisp_vals = vals_all[63:]\n",
    "    \n",
    "    \n",
    "    orig = pd.DataFrame({'Nonhispanic':orig_nonhisp_vals, 'Hispanic':orig_hisp_vals})\n",
    "    \n",
    "    new = pd.read_csv(file)\n",
    "    new = new.loc[:, ~new.columns.str.contains('^Unnamed')]\n",
    "       \n",
    "    \n",
    "    #get percent race occurrences of each race in original data (only calculate for minorites >5%)\n",
    "    #iterate over columns to find total\n",
    "    total = 0\n",
    "    for index, row in orig.iterrows():\n",
    "        total = total + row['Hispanic']+row['Nonhispanic']\n",
    "        \n",
    "    final = new.copy()\n",
    "    for col in final.columns:\n",
    "        final[col].values[:] = 0\n",
    "        \n",
    "    #find mean difference for all minority groups less than threshold\n",
    "    for index, row in new.iterrows():\n",
    "        if (row['Hispanic']/total <= threshold) and (row['Hispanic']-orig.at[index, 'Hispanic']>0):\n",
    "            #CAN I DO THIS\n",
    "            final.at[index, 'Hispanic']= abs(new.at[index,'Hispanic']-orig.at[index,'Hispanic']/(orig.at[index,'Hispanic']+1))\n",
    "        if row['Nonhispanic']/total <= threshold and (row['Nonhispanic']-orig.at[index, 'Nonhispanic']>0):\n",
    "            final.at[index, 'Nonhispanic']= abs(new.at[index,'Nonhispanic']-orig.at[index,'Nonhispanic']/(orig.at[index,'Nonhispanic']+1))\n",
    "    \n",
    "    mean = (final['Nonhispanic'].sum()+final['Hispanic'].sum())/np.count_nonzero(final)\n",
    "        \n",
    "    return mean\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alameda\n",
      "dprun_0.01_0.csv\n",
      "dprun_0.11_0.csv\n",
      "dprun_0.21_0.csv\n",
      "dprun_0.31_0.csv\n",
      "dprun_0.41_0.csv\n",
      "dprun_0.51_0.csv\n",
      "dprun_0.61_0.csv\n",
      "dprun_0.71_0.csv\n",
      "dprun_0.81_0.csv\n",
      "dprun_0.91_0.csv\n",
      "dprun_1.01_0.csv\n",
      "dprun_1.11_0.csv\n",
      "dprun_1.21_0.csv\n",
      "dprun_1.31_0.csv\n",
      "dprun_1.41_0.csv\n",
      "dprun_1.51_0.csv\n",
      "dprun_1.61_0.csv\n",
      "dprun_1.71_0.csv\n",
      "dprun_1.81_0.csv\n",
      "dprun_1.91_0.csv\n",
      "dprun_2.01_0.csv\n",
      "dprun_2.11_0.csv\n",
      "dprun_2.21_0.csv\n",
      "dprun_2.31_0.csv\n",
      "dprun_2.41_0.csv\n",
      "dprun_2.51_0.csv\n",
      "dprun_2.61_0.csv\n",
      "dprun_2.71_0.csv\n",
      "dprun_2.81_0.csv\n",
      "dprun_2.91_0.csv\n",
      "dprun_3.01_0.csv\n",
      "dprun_3.11_0.csv\n",
      "dprun_3.21_0.csv\n",
      "dprun_3.31_0.csv\n",
      "dprun_3.41_0.csv\n",
      "dprun_3.51_0.csv\n",
      "dprun_3.61_0.csv\n",
      "dprun_3.71_0.csv\n",
      "dprun_3.81_0.csv\n",
      "dprun_3.91_0.csv\n",
      "dprun_4.01_0.csv\n",
      "dprun_4.11_0.csv\n",
      "dprun_4.21_0.csv\n",
      "dprun_4.31_0.csv\n",
      "dprun_4.41_0.csv\n",
      "dprun_4.51_0.csv\n",
      "dprun_4.61_0.csv\n",
      "dprun_4.71_0.csv\n",
      "dprun_4.81_0.csv\n",
      "dprun_4.91_0.csv\n",
      "dprun_5.01_0.csv\n",
      "dprun_5.11_0.csv\n",
      "dprun_5.21_0.csv\n",
      "dprun_5.31_0.csv\n",
      "dprun_5.41_0.csv\n",
      "dprun_5.51_0.csv\n",
      "dprun_5.61_0.csv\n",
      "dprun_5.71_0.csv\n",
      "dprun_5.81_0.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-cb090e915a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcounty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mdp_mean_dif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-cb090e915a4a>\u001b[0m in \u001b[0;36mdp_mean_dif\u001b[0;34m(county)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mmeandif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_minor_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../dp/newdp_runs/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcounty\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mdfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mean_difference'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmeandif\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-e095de4e445f>\u001b[0m in \u001b[0;36mget_minor_dp\u001b[0;34m(county, file, threshold)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mdf_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'^Unnamed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mdf_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SwapVal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'race'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hispanic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m             \u001b[0minferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minferred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"interval\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"period\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.infer_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib._try_infer_map\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;31m# historically, void subclasses preserve their name, eg `record64`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def dp_mean_dif(county):\n",
    "    \n",
    "    ep = np.arange(.01, 10, 1, float)\n",
    "    \n",
    "    epsilon = []\n",
    "    \n",
    "    for ee in ep:\n",
    "        ee = str(round(ee,1))\n",
    "        if ee[len(ee)-1] == '.':\n",
    "            ee = str(e)+'0'\n",
    "        epsilon.append(ee)\n",
    "        \n",
    "    dfinal = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "\n",
    "    for filename in os.listdir('../dp/newdp_runs/'+county+'/'):\n",
    "            if filename.endswith(\".csv\") and '_0.csv' in filename:\n",
    "                print(filename)\n",
    "                s = 0\n",
    "                meandif = get_minor_dp(county, '../dp/newdp_runs/'+county+'/'+filename, .05)\n",
    "                \n",
    "                dfinal = dfinal.append({'filename': filename, 'mean_difference': meandif}, ignore_index=True)\n",
    "                \n",
    "    sorted = {}\n",
    "    for e in epsilon:\n",
    "        #meandif, count\n",
    "        sorted[e] = []\n",
    "\n",
    "    for column_name, data in dfinal.iterrows():\n",
    "        f = data[0].rfind('_')\n",
    "        swap = data[0][0:f]\n",
    "        swap = swap[6:]\n",
    "        \n",
    "        \n",
    "        for e in epsilon:\n",
    "            if swap[:3] == e[:3]: \n",
    "                sorted[e].append(data[1])\n",
    "                break\n",
    "    \n",
    "    for e in epsilon:\n",
    "        sorted[e] = sum(sorted[e])/len(sorted[e])\n",
    "                \n",
    "    final = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "    for e in sorted:\n",
    "        final = final.append({'filename': e, 'mean_difference': sorted[e]}, ignore_index=True)\n",
    "                \n",
    "\n",
    "    fa = \"mean_dif/\"+county+\"/dp/dpnew.csv\"\n",
    "    csv_orig_data = final.to_csv(fa, index = True)\n",
    "\n",
    "    \n",
    "counties = ['Alameda', 'Armstrong', 'Cibola', 'Fayette', 'GrandForks', 'Hawaii', 'Jefferson', 'Nantucket', 'Washington']\n",
    "counties = ['alameda']\n",
    "for county in counties:\n",
    "    print(county)\n",
    "    dp_mean_dif(county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find races that are <10% in dataset\n",
    "\n",
    "def get_minor_swapping(county, file, threshold, mu):\n",
    "    df_orig = pd.read_csv (r'../homemade_data/'+county+'.csv')\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('^Unnamed')]\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('SwapVal')]\n",
    "    for index, row in df_orig.iterrows():\n",
    "        d = row['race']\n",
    "        h = row['hispanic']\n",
    "        if d in races and h==1:\n",
    "            df_orig.at[index, 'race'] = races.index(d)+63\n",
    "        elif d in races and h==0:\n",
    "            df_orig.at[index, 'race'] = races.index(d)\n",
    "    labels_all = []\n",
    "    vals_all = []\n",
    "\n",
    "    #get list of occurrences by race - orig\n",
    "    for key1, value1 in df_orig.iteritems():\n",
    "        if(key1 == 'race'):\n",
    "            labs, vals = zip(*Counter(value1).items())\n",
    "            labs = list(labs)\n",
    "            vals = list(vals)\n",
    "            for i in range(0, len(real_order)):\n",
    "                if i not in labs:\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(0)\n",
    "                elif i in labs:\n",
    "                    pos = labs.index(i)\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(vals[pos])   \n",
    "    labels_all = np.arange(0,len(real_order),1)\n",
    "    \n",
    "    orig_nonhisp_vals = vals_all[0:63]\n",
    "    orig_hisp_vals = vals_all[63:]\n",
    "    \n",
    "    orig = pd.DataFrame({'Nonhispanic':orig_nonhisp_vals, 'Hispanic':orig_hisp_vals})\n",
    "    \n",
    "       \n",
    "    #get list of occurences by race - swapping\n",
    "    df2 = pd.read_csv(file)\n",
    "    for index, row in df2.iterrows():\n",
    "        d = row['race']\n",
    "        h = row['hispanic']\n",
    "        if h==1:\n",
    "            df2.at[index, 'race'] = d+63\n",
    "    labels2_all = []\n",
    "    vals2_all = []\n",
    "    \n",
    "    for key, value in df2.iteritems():\n",
    "        if(key == 'race'):\n",
    "            labels2, values2 = zip(*Counter(value).items())\n",
    "            labels2=list(labels2)\n",
    "            values2 = list(values2)\n",
    "            for i in range(0, len(real_order)):\n",
    "                if i not in labels2:\n",
    "                    labels2_all.append(i)\n",
    "                    vals2_all.append(0)\n",
    "                elif i in labels2:\n",
    "                    pos = labels2.index(i)\n",
    "                    labels2_all.append(i)\n",
    "                    vals2_all.append(values2[pos])\n",
    "    labels2_all = np.arange(0,len(real_order),1)\n",
    "    \n",
    "    new_nonhisp_vals = vals2_all[0:63]\n",
    "    new_hisp_vals = vals2_all[63:]\n",
    "    \n",
    "    new = pd.DataFrame({'Nonhispanic':new_nonhisp_vals, 'Hispanic':new_hisp_vals})\n",
    "    \n",
    "\n",
    "    #get percent race occurrences of each race in original data (only calculate for minorites >5%)\n",
    "    #iterate over columns to find total\n",
    "    total = 0\n",
    "    total_nonhisp = 0\n",
    "    for index, row in orig.iterrows():\n",
    "        total = total + row['Hispanic']+row['Nonhispanic']\n",
    "        total_nonhisp += row['Nonhispanic']\n",
    "        \n",
    "    final = orig.copy()\n",
    "    for col in final.columns:\n",
    "        final[col].values[:] = 0\n",
    "       \n",
    "    orig_nonhisp_vals.extend(orig_hisp_vals)\n",
    "    new_nonhisp_vals.extend(new_hisp_vals)\n",
    "#     print(orig_nonhisp_vals)\n",
    "#     print(orig_nonhisp_vals/total_nonhisp)\n",
    "        \n",
    "    return Dkl(mu, orig_nonhisp_vals, new_nonhisp_vals)\n",
    "    \n",
    "#     #find mean difference for all minority groups less than threshold\n",
    "#     for index, row in new.iterrows():\n",
    "#         if row['Hispanic']/total <= threshold and (row['Hispanic']-orig.at[index, 'Hispanic'] >0):\n",
    "#             #CAN I DO THIS\n",
    "#             final.at[index, 'Hispanic']= abs(new.at[index,'Hispanic']-orig.at[index,'Hispanic']/(orig.at[index,'Hispanic']+1))\n",
    "#         if row['Nonhispanic']/total <= threshold and (row['Nonhispanic']-orig.at[index, 'Nonhispanic']>0):\n",
    "#             final.at[index, 'Nonhispanic']= abs(new.at[index,'Nonhispanic']-orig.at[index,'Nonhispanic']/(orig.at[index,'Nonhispanic']+1))\n",
    "    \n",
    "#     mean = (final['Nonhispanic'].sum()+final['Hispanic'].sum())/np.count_nonzero(final)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_mean_dif(county):\n",
    "    dfinal = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "\n",
    "    for filename in os.listdir('../dp/newdp_runs'+county):\n",
    "            if filename.endswith(\".csv\") and '_0.csv' in filename:\n",
    "                print(filename)\n",
    "                meandif = get_minor_swapping(county, '../dp/newdp_runs/'+county+filename, .05, 1)\n",
    "                \n",
    "                dfinal = dfinal.append({'filename': filename, 'mean_difference': meandif}, ignore_index=True)\n",
    "                \n",
    "    swaprates = np.arange(.01, .5, .05, float)\n",
    "    \n",
    "    sorted = {}\n",
    "    for s in swaprates:\n",
    "        #meandif, count\n",
    "        sorted[s] = []\n",
    "   \n",
    "    for column_name, data in dfinal.iterrows():\n",
    "        f = data[0].rfind('_')\n",
    "        swap = data[0][0:f]\n",
    "        swap = swap[5:]\n",
    "        \n",
    "        for s in swaprates:\n",
    "            spos = str(s)[0:f]\n",
    "            if swap == spos:           \n",
    "                sorted[s].append(data[1])\n",
    "                break\n",
    "    \n",
    "    for s in swaprates:\n",
    "        sorted[s] = sum(sorted[s])/len(sorted[s])\n",
    "                \n",
    "    final = pd.DataFrame(columns=['filename', 'mean_difference'])\n",
    "    for s in sorted:\n",
    "        final = final.append({'filename': s, 'mean_difference': sorted[s]}, ignore_index=True)\n",
    "                \n",
    "        \n",
    "    \n",
    "    fa = \"mean_dif/\"+county+\"/similar/newswapping.csv\"\n",
    "    csv_orig_data = final.to_csv(fa, index = True)\n",
    "\n",
    "    \n",
    "counties = ['Alameda', 'Armstrong', 'Cibola', 'Fayette', 'GrandForks', 'Hawaii', 'Jefferson', 'Nantucket', 'Washington']\n",
    "counties = ['alameda']\n",
    "for county in counties:\n",
    "    print(county)\n",
    "    similar_mean_dif(county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21923942623264103\n",
      "0.20630434587085927\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "p = [.1, .2, .3, .4]\n",
    "q = [.11, .18, .35, .2, .68]\n",
    "print(Dkl(.1, p, q))\n",
    "print(Dkl(.2, p, q))\n",
    "print(Dkl(.1, p, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minor_dp(county, file, threshold, mu):\n",
    "    df_orig = pd.read_csv (r'../homemade_data/'+county+'.csv')\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('^Unnamed')]\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('SwapVal')]\n",
    "    for index, row in df_orig.iterrows():\n",
    "        d = row['race']\n",
    "        h = row['hispanic']\n",
    "        if d in races and h==1:\n",
    "            df_orig.at[index, 'race'] = races.index(d)+63\n",
    "        elif d in races and h==0:\n",
    "            df_orig.at[index, 'race'] = races.index(d)\n",
    "    labels_all = []\n",
    "    vals_all = []\n",
    "\n",
    "    #get list of occurrences by race - orig\n",
    "    for key1, value1 in df_orig.iteritems():\n",
    "        if(key1 == 'race'):\n",
    "            labs, vals = zip(*Counter(value1).items())\n",
    "            labs = list(labs)\n",
    "            vals = list(vals)\n",
    "            for i in range(0, len(real_order)):\n",
    "                if i not in labs:\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(0)\n",
    "                elif i in labs:\n",
    "                    pos = labs.index(i)\n",
    "                    labels_all.append(i)\n",
    "                    vals_all.append(vals[pos])   \n",
    "    labels_all = np.arange(0,len(real_order),1)\n",
    "    \n",
    "    orig_nonhisp_vals = vals_all[0:63]\n",
    "    orig_hisp_vals = vals_all[63:]\n",
    "    \n",
    "    orig = pd.DataFrame({'Nonhispanic':orig_nonhisp_vals, 'Hispanic':orig_hisp_vals})\n",
    "    \n",
    "       \n",
    "    #get list of occurences by race - swapping\n",
    "    q_histogram = [0 for i in range(126)]\n",
    "    df2 = pd.read_csv(file)\n",
    "    for index, row in df2.iterrows():\n",
    "        q_histogram[index] = row['Nonhispanic']\n",
    "        q_histogram[index+63] = row['Hispanic']\n",
    "    q_histogram = np.array(q_histogram)\n",
    "\n",
    "    \n",
    "    #     labels2_all = []\n",
    "#     vals2_all = []\n",
    "    \n",
    "    \n",
    "#     for key, value in df2.iteritems():\n",
    "#         if(key == 'race'):\n",
    "#             labels2, values2 = zip(*Counter(value).items())\n",
    "#             labels2=list(labels2)\n",
    "#             values2 = list(values2)\n",
    "#             for i in range(0, len(real_order)):\n",
    "#                 if i not in labels2:\n",
    "#                     labels2_all.append(i)\n",
    "#                     vals2_all.append(0)\n",
    "#                 elif i in labels2:\n",
    "#                     pos = labels2.index(i)\n",
    "#                     labels2_all.append(i)\n",
    "#                     vals2_all.append(values2[pos])\n",
    "#     labels2_all = np.arange(0,len(real_order),1)\n",
    "    \n",
    "\n",
    "#     new_nonhisp_vals = vals2_all[0:63]\n",
    "#     new_hisp_vals = vals2_all[63:]\n",
    "    \n",
    "#     new = pd.DataFrame({'Nonhispanic':new_nonhisp_vals, 'Hispanic':new_hisp_vals})\n",
    "    \n",
    "\n",
    "    #get percent race occurrences of each race in original data (only calculate for minorites >5%)\n",
    "    #iterate over columns to find total\n",
    "    total = 0\n",
    "    total_nonhisp = 0\n",
    "    for index, row in orig.iterrows():\n",
    "        total = total + row['Hispanic']+row['Nonhispanic']\n",
    "        total_nonhisp += row['Nonhispanic']\n",
    "        \n",
    "    final = orig.copy()\n",
    "    for col in final.columns:\n",
    "        final[col].values[:] = 0\n",
    "       \n",
    "    orig_nonhisp_vals.extend(orig_hisp_vals)\n",
    "#     new_nonhisp_vals.extend(new_hisp_vals)\n",
    "    q_histogram_norm = np.abs(q_histogram)\n",
    "    orig_nonhisp_vals = np.array(orig_nonhisp_vals)\n",
    "#     print(q_histogram/np.sum(q_histogram))\n",
    "#     print(orig_nonhisp_vals/total_nonhisp)\n",
    "#     print(orig_nonhisp_vals - q_histogram_norm)\n",
    "    #return Dkl(mu, orig_nonhisp_vals/total, q_histogram_norm/np.sum(q_histogram_norm))\n",
    "    return Dkl(mu, orig_nonhisp_vals/total, q_histogram_norm/np.sum(q_histogram_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon: 0.51         div: 0.1439287170663931\n",
      "epsilon: 1.01         div: 0.0732147148046224\n",
      "epsilon: 2.01         div: 0.026395580625801825\n",
      "epsilon: 3.01         div: 0.009387975909375075\n",
      "epsilon: 4.01         div: 0.004194833728742649\n",
      "epsilon: 5.01         div: 0.0004916877272229807\n",
      "epsilon: 6.01         div: 0.0014739582454638335\n",
      "epsilon: 7.01         div: 0.0\n",
      "epsilon: 8.01         div: 0.0\n",
      "epsilon: 9.01         div: 0.0\n"
     ]
    }
   ],
   "source": [
    "county = 'alameda'\n",
    "xs = np.arange(0, 10, .1)\n",
    "ys = []\n",
    "eps = ['0.51', '1.01', '2.01', '3.01', '4.01', '5.01', '6.01', '7.01', '8.01', '9.01']\n",
    "# for filename in os.listdir('../dp/newdp_runs/'+county):\n",
    "#     if filename.endswith(\".csv\") and '_0.csv' in filename:\n",
    "#         print(filename)\n",
    "#         div = get_minor_dp(county, '../dp/newdp_runs/'+county+'/' +filename, 1, .1)\n",
    "        \n",
    "#         print(div)\n",
    "#         ys.append(div)\n",
    "\n",
    "# eps = ['100', '200', '500', '1000']\n",
    "for ep in eps:\n",
    "    div = get_minor_dp(county, '../dp/newdp_runs2/'+county+'/dprun_' + ep + '_0.csv', 1, .0001)\n",
    "    print('epsilon: ' + ep + '         ' + 'div: ' + str(div))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7117976891535903, 0.6652906798836871, 0.5563304295837747, 0.4968107755530724, 0.49451313785805634, 0.4057724221293193, 0.3454613574129084, 0.3204875992002956, 0.2745387413544879, 0.2855807061798907, 0.2803352525000124, 0.21346181674203354]\n"
     ]
    }
   ],
   "source": [
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48075577 0.17984605 0.00279916 0.07977607 0.         0.00069979\n",
      " 0.00769769 0.00769769 0.0139958  0.00069979 0.00419874 0.00279916\n",
      " 0.00489853 0.         0.         0.00069979 0.         0.00069979\n",
      " 0.00069979 0.00209937 0.         0.00559832 0.00279916 0.\n",
      " 0.00069979 0.00069979 0.00139958 0.         0.         0.00139958\n",
      " 0.         0.00069979 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00209937\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.13575927 0.00489853 0.0069979\n",
      " 0.00069979 0.         0.02939118 0.00279916 0.00279916 0.00559832\n",
      " 0.         0.00069979 0.00069979 0.         0.         0.00069979\n",
      " 0.00139958 0.         0.00069979 0.         0.         0.\n",
      " 0.00069979 0.         0.         0.         0.00069979 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.59687228 0.2232841  0.00347524 0.09904431 0.         0.00086881\n",
      " 0.00955691 0.00955691 0.01737619 0.00086881 0.00521286 0.00347524\n",
      " 0.00608167 0.         0.         0.00086881 0.         0.00086881\n",
      " 0.00086881 0.00260643 0.         0.00695048 0.00347524 0.\n",
      " 0.00086881 0.00086881 0.00173762 0.         0.         0.00173762\n",
      " 0.         0.00086881 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00260643\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.16854909 0.00608167 0.0086881\n",
      " 0.00086881 0.         0.03649001 0.00347524 0.00347524 0.00695048\n",
      " 0.         0.00086881 0.00086881 0.         0.         0.00086881\n",
      " 0.00173762 0.         0.00086881 0.         0.         0.\n",
      " 0.00086881 0.         0.         0.         0.00086881 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "eps = ['100', '200', '500', '1000']\n",
    "eps = ['100000000']\n",
    "for ep in eps:\n",
    "    div = get_minor_dp(county, '../dp/newdp_runs/'+county+'/dprun_' + ep + '_0.csv', 1, .001)\n",
    "    print(div)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
