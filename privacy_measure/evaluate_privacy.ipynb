{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "\n",
    "#retrieve race keys\n",
    "%store -r races\n",
    "%store -r races2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "#CREATE_DATA: LOAD IN DATASET\n",
    "###########################################################################################################################\n",
    "# INPUT: \n",
    "#      path: path csv containing block data\n",
    "#      age_buckets: value for number of items in each age bucket\n",
    "\n",
    "#  -reads in csv file to pandas dataframe\n",
    "#  -replaces race string with equivalent ints 0-63 race if applicable\n",
    "#  -replaces ages with appropriate age bucket labels if applicable\n",
    "\n",
    "# OUTPUT: dataframe containing dataset with modified age and race values\n",
    "##########################################################################################################################\n",
    "\n",
    "def create_data(path, age_groupsize, max_age):\n",
    "    #import original data\n",
    "    df_orig = pd.read_csv (path)\n",
    "    df_orig = df_orig.loc[:, ~df_orig.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    #get size of buckets based upon group size (if groupsize of 5, how many group of 5 buckets up to max age?)\n",
    "    bucketsize = math.ceil(max_age/age_groupsize)\n",
    "    \n",
    "    #adjust values of necessary\n",
    "    for index, row in df_orig.iterrows():    \n",
    "        \n",
    "        #RACE\n",
    "        d = row['race']\n",
    "        h = row['hispanic']\n",
    "        #replace race w/ value 0-63\n",
    "        if d in races:\n",
    "            df_orig.at[index, 'race'] = races.index(d)\n",
    "        elif d in races2:\n",
    "            df_orig.at[index, 'race'] = races2.index(d)        \n",
    "            \n",
    "        #AGE\n",
    "        #if age buckets are implemented:\n",
    "        if age_groupsize > 1: \n",
    "            a = row['age']\n",
    "            #get number of times groupsize goes into age:\n",
    "            #ex. get new value for 12 with a group size of 5:\n",
    "            #    12/5 = 2 w/remainder of 3 --> would return 2\n",
    "            #set value in original dataset to this age bucket value\n",
    "            df_orig.at[index, 'age'] = np.floor(a/age_groupsize)\n",
    "                 \n",
    "    return df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "#CREATE ATTRIBUTE COMBOS TO LOOK FOR IN DATASETS\n",
    "###########################################################################################################################\n",
    "#INPUT:\n",
    "#     age_range: all possible values for age\n",
    "#     race_range: all possible values for race\n",
    "#     size_range: all possible values for household_size\n",
    "\n",
    "#  -create all possible combos of age, race, household size\n",
    "\n",
    "#OUTPUT:\n",
    "#  -M: list containing all age, race, household size combos\n",
    "###########################################################################################################################\n",
    "\n",
    "def create_combos(age_range, race_range, size_range):\n",
    "    \n",
    "    #list containing all age, race, household size combos\n",
    "    M = []\n",
    "    \n",
    "    #create all combos; append to M\n",
    "    for age in age_range:\n",
    "            for race in race_range:\n",
    "                for size in size_range:\n",
    "                    M.append([age, race, size])\n",
    "                    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "#FIND UNIQUE ROWS\n",
    "###########################################################################################################################\n",
    "#INPUT:\n",
    "#     M: set of attributes shared between D and P ['age', 'race', 'sex']\n",
    "#     D: dataframe to search for unique rows\n",
    "#     theta_d: attribute set size limit in D. we only examine attribute tuples with at most theta_d matching entries in D\n",
    "\n",
    "#  -get how many times each attribute combination occurs in the dataframe\n",
    "#  -if only occurs once, add id to unique list; if occurs under the given theta_d threshold, add id to under_threshold list\n",
    "#  -return these lists of rows that are unique or under threshold \n",
    "\n",
    "#OUTPUT:\n",
    "#  -unique: list of ids of rows that are unique in the dataset\n",
    "#  -under_threshold: list of ids of rows that are under the given occurence threshold in the dataset\n",
    "###########################################################################################################################\n",
    "    \n",
    "def get_unique_rows(M, D, theta_d):\n",
    "\n",
    "    #vals to be returned:\n",
    "    #1. unique rows in D \n",
    "    unique = []\n",
    "    #2. those under theta_d threshold\n",
    "    under_threshold = []\n",
    "   \n",
    "    #for attribute combination in M\n",
    "    for row in M:   \n",
    "        \n",
    "        #true case - get how many times that the attribute combination occurs in the deidentified dataset \n",
    "        att_val_combos = D.loc[(D['age'] == row[0]) & (D['race'] == row[1]) & (D['household_size'] == row[2])]\n",
    "        \n",
    "        #if combo occurs at least once and less than threshold in deidentified dataset\n",
    "        if len(att_val_combos) <= theta_d and len(att_val_combos) > 0:\n",
    "            #add IDs to under_threshold list\n",
    "            for index, row in att_val_combos.iterrows():\n",
    "                under_threshold.append([row['id'], [row['age'], row['race'], row['household_size']]])\n",
    "            \n",
    "        if len(att_val_combos) == 1:\n",
    "            #add IDs to unique list\n",
    "            for index, row in att_val_combos.iterrows():\n",
    "                unique.append([row['id'], [row['age'], row['race'], row['household_size']]])\n",
    "    \n",
    "    return unique, under_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "#FIND ROW MATCHES\n",
    "###########################################################################################################################\n",
    "#INPUT:\n",
    "#     public_dataset: publically published dataset\n",
    "#     P_unique: ids of unique rows in public dataset\n",
    "#     de_id_dataset: deidentified dataset\n",
    "#     D_unique: ids of unique rows in de-identified dataset\n",
    "\n",
    "#  -find all unique rows with matching row values\n",
    "#  -return ids of matches\n",
    "\n",
    "#OUTPUT:\n",
    "#     matches: list of ids of all matching unique values [id in public, id in de_id]\n",
    "###########################################################################################################################\n",
    "def find_matches(P_unique, D_unique):\n",
    "    \n",
    "    #create list of unique row values only\n",
    "    D_unique_rows = []\n",
    "    for d in D_unique:\n",
    "        D_unique_rows.append(d[1])\n",
    "        \n",
    "    #LIST OF ROWS WITH MATCHES\n",
    "    matches = []\n",
    "    \n",
    "    #for each row in P_unique, see if there's a matching row in D_unique\n",
    "    for p_row in P_unique:\n",
    "        #see if values of [age, race, sex] associated with unique row in p (p_row) are also a unique row in D\n",
    "        if p_row[1] in D_unique_rows:\n",
    "            # if so, find associated ID\n",
    "            for d_row in D_unique:\n",
    "                if d_row[1] == p_row[1]:\n",
    "                    matches.append([p_row[0], d_row[0]])\n",
    "                    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "#CHECK VALIDITY OF MATCHES\n",
    "###########################################################################################################################\n",
    "#INPUT:\n",
    "#     public_dataset: publically published dataset\n",
    "#     de_id_dataset: deidentified dataset\n",
    "\n",
    "#  -for each match, check if var predicted is the same, and if vars are the same\n",
    "\n",
    "#OUTPUT:\n",
    "#     correct_matches: matches with same id (meaning the same person's data)\n",
    "#     correct_values: all reported matches that had same values (not necessarily an exact ID match)\n",
    "###########################################################################################################################\n",
    "\n",
    "\n",
    "def check_true_vals(matches, public_dataset, de_id_dataset):\n",
    "    correct_matches = []\n",
    "    correct_values = []\n",
    "    \n",
    "    #for each match\n",
    "    for match in matches:\n",
    "        \n",
    "        #get values for p_id and d_id\n",
    "        p_id = match[0]\n",
    "        d_id = match[1]\n",
    "        \n",
    "        #get row values are all the same for p_id row and d_id row\n",
    "        p_row = public_dataset.loc[public_dataset['id'] == d_id]\n",
    "        d_row = de_id_dataset.loc[de_id_dataset['id'] == p_id]\n",
    "        #check if all relevant column values match for p_id and d_id rows\n",
    "        if ((p_row.iloc[0]['hispanic']==d_row.iloc[0]['hispanic'])):\n",
    "            #if all values match, add to correct_values\n",
    "            correct_values.append(match)\n",
    "        if p_row.iloc[0]['id']==d_row.iloc[0]['id']:\n",
    "            #if ids match, add to correct_matches\n",
    "            correct_matches.append(match)   \n",
    "    \n",
    "    return correct_matches, correct_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_privacy(attribute_ranges, original_data_path, de_id_data_directory, age_groupsize, theta_d, k1, k2):\n",
    "    \n",
    "    correct = []\n",
    "    total = []\n",
    "    \n",
    "    #create list of all combinations of attributes that could be present in either the public or de-identified dataset\n",
    "    attribute_combos = create_combos(attribute_ranges[0], attribute_ranges[1], attribute_ranges[2])\n",
    "\n",
    "    #create public dataset\n",
    "    public_dataset = create_data(original_data_path, age_groupsize, len(attribute_ranges[0])-1)\n",
    "        \n",
    "    #find unique rows in public dataset\n",
    "    P_unique, P_under_threshold = get_unique_rows(attribute_combos, public_dataset, theta_d)\n",
    "    \n",
    "    #these are the swap rates to run privacy analysis on\n",
    "    swaprates = np.arange(k1, k2, .01, float).tolist()\n",
    "    \n",
    "    #for each de-id in given directory:\n",
    "    for swaprate in swaprates:\n",
    "        \n",
    "        correct_matches_all =[]\n",
    "        total_matches_all = []\n",
    "        \n",
    "        #for all de-id files in directory:\n",
    "        for filename in os.listdir(de_id_data_directory):\n",
    "            \n",
    "            #check if file is a match for the given swaprate:\n",
    "            swap_file = filename[(filename.find(\"_\"))+1:filename.rfind(\"_\")]\n",
    "            \n",
    "            #if match:\n",
    "            if filename.endswith(\".csv\") and math.isclose(float(swaprate), float(swap_file)) and \"_0.csv\" in filename:\n",
    "                \n",
    "                #create deid-ed dataset \n",
    "                de_id_dataset = create_data(de_id_data_directory+filename, age_groupsize, len(attribute_ranges[0])-1)\n",
    "                \n",
    "                #find unique rows in de-ided\n",
    "                D_unique, D_under_threshold = get_unique_rows(attribute_combos, de_id_dataset, theta_d)\n",
    "\n",
    "                #find matches for unique rows\n",
    "                matches = find_matches(P_unique, D_unique)\n",
    "\n",
    "                #use ID to check true values (predict var & see if var is correct)\n",
    "                correct_matches, total_matches = check_true_vals(matches, public_dataset, de_id_dataset)\n",
    "\n",
    "                correct_matches_all.append(correct_matches)\n",
    "                total_matches_all.append(total_matches)\n",
    "        \n",
    "        correct_matches_total = [0,0]\n",
    "        total_matches_total = [0,0]\n",
    "\n",
    "        for s in correct_matches_all:\n",
    "            correct_matches_total[0] = correct_matches_total[0] + len(s)\n",
    "            correct_matches_total[1] = correct_matches_total[1] + 1\n",
    "\n",
    "        for s in total_matches_all:\n",
    "            total_matches_total[0] = total_matches_total[0] + len(s)\n",
    "            total_matches_total[1] = total_matches_total[1] + 1\n",
    "\n",
    "        corr_matches = correct_matches_total[0]/correct_matches_total[1]\n",
    "        tot_matches = total_matches_total[0]/total_matches_total[1]\n",
    "\n",
    "        correct.append([swaprate, corr_matches])\n",
    "        total.append([swaprate, tot_matches])\n",
    "        \n",
    "        print([swaprate, corr_matches], [swaprate, tot_matches])\n",
    "    \n",
    "    return correct, total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 82.0] [0.01, 82.0]\n",
      "[0.02, 80.0] [0.02, 80.0]\n",
      "[0.03, 76.0] [0.03, 76.0]\n",
      "[0.04, 69.0] [0.04, 69.0]\n",
      "[0.05, 73.0] [0.05, 73.0]\n",
      "[0.060000000000000005, 66.0] [0.060000000000000005, 66.0]\n",
      "[0.06999999999999999, 58.0] [0.06999999999999999, 59.0]\n",
      "[0.08, 57.0] [0.08, 58.0]\n",
      "[0.09, 52.0] [0.09, 52.0]\n",
      "[0.09999999999999999, 57.0] [0.09999999999999999, 57.0]\n",
      "[0.11, 53.0] [0.11, 55.0]\n",
      "[0.12, 44.0] [0.12, 45.0]\n",
      "[0.13, 40.0] [0.13, 41.0]\n",
      "[0.14, 43.0] [0.14, 43.0]\n",
      "[0.15000000000000002, 33.0] [0.15000000000000002, 33.0]\n",
      "[0.16, 32.0] [0.16, 34.0]\n",
      "[0.17, 31.0] [0.17, 33.0]\n",
      "[0.18000000000000002, 30.0] [0.18000000000000002, 32.0]\n",
      "[0.19, 23.0] [0.19, 27.0]\n",
      "[0.2, 26.0] [0.2, 29.0]\n",
      "[0.21000000000000002, 18.0] [0.21000000000000002, 25.0]\n",
      "[0.22, 17.0] [0.22, 22.0]\n",
      "[0.23, 16.0] [0.23, 23.0]\n",
      "[0.24000000000000002, 17.0] [0.24000000000000002, 22.0]\n",
      "[0.25, 14.0] [0.25, 18.0]\n",
      "[0.26, 14.0] [0.26, 18.0]\n",
      "[0.27, 13.0] [0.27, 19.0]\n",
      "[0.28, 13.0] [0.28, 16.0]\n",
      "[0.29000000000000004, 20.0] [0.29000000000000004, 24.0]\n",
      "[0.3, 16.0] [0.3, 24.0]\n",
      "[0.31, 14.0] [0.31, 22.0]\n",
      "[0.32, 9.0] [0.32, 14.0]\n",
      "[0.33, 14.0] [0.33, 21.0]\n",
      "[0.34, 17.0] [0.34, 24.0]\n",
      "[0.35000000000000003, 15.0] [0.35000000000000003, 22.0]\n",
      "[0.36000000000000004, 14.0] [0.36000000000000004, 17.0]\n",
      "[0.37, 16.0] [0.37, 21.0]\n",
      "[0.38, 17.0] [0.38, 21.0]\n",
      "[0.39, 15.0] [0.39, 22.0]\n",
      "[0.4, 13.0] [0.4, 17.0]\n",
      "[0.41000000000000003, 14.0] [0.41000000000000003, 20.0]\n",
      "[0.42000000000000004, 13.0] [0.42000000000000004, 15.0]\n",
      "[0.43, 10.0] [0.43, 12.0]\n",
      "[0.44, 13.0] [0.44, 16.0]\n",
      "[0.45, 8.0] [0.45, 12.0]\n",
      "[0.46, 9.0] [0.46, 13.0]\n",
      "[0.47000000000000003, 14.0] [0.47000000000000003, 22.0]\n",
      "[0.48000000000000004, 11.0] [0.48000000000000004, 17.0]\n"
     ]
    }
   ],
   "source": [
    "#RUNNER FUNCTION\n",
    "\n",
    "def runner_function(county, swapping_directory_type, age_groupsize, theta_d, start_rate, stop_rate):\n",
    "    attribute_ranges = [range(90), range(len(races)), [1,2,3,4]]\n",
    "    correct, total = evaluate_privacy(attribute_ranges, '../homemade_data/' + county + '.csv', \n",
    "                 '../swapping/swap_runs/'+county+'/'+swapping_directory_type+'/',\n",
    "                 age_groupsize, theta_d, start_rate, stop_rate)\n",
    "    with open(county+\"_\"+swapping_directory_type+\"_\"+age_groupsize+'_privacy.txt', \"a\") as f:\n",
    "        f.write(\"Correct:\\n\\n\")\n",
    "        for c in correct:\n",
    "            f.write(c + \"\\n\")\n",
    "        f.write(\"Total:\\n\\n\")\n",
    "        for t in total:\n",
    "            f.write(t + \"\\n\")\n",
    "                     \n",
    "                \n",
    "runner_function('alameda', 'similar_0', 15, 3, 0.01, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
