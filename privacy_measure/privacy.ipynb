{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now analyzing  alameda\n",
      "swap rate: 0.01\n",
      "swap_0.01_0.csv\n",
      "swap rate: 0.02\n",
      "swap_0.02_0.csv\n",
      "swap rate: 0.03\n",
      "swap_0.03_0.csv\n",
      "swap rate: 0.04\n",
      "swap_0.04_0.csv\n",
      "swap rate: 0.05\n",
      "swap_0.05_0.csv\n",
      "swap rate: 0.060000000000000005\n",
      "swap_0.060000000000000005_0.csv\n",
      "swap rate: 0.06999999999999999\n",
      "swap_0.06999999999999999_0.csv\n",
      "swap rate: 0.08\n",
      "swap_0.08_0.csv\n",
      "swap rate: 0.09\n",
      "swap_0.09_0.csv\n",
      "swap rate: 0.09999999999999999\n",
      "swap_0.09999999999999999_0.csv\n",
      "swap rate: 0.11\n",
      "swap_0.11_0.csv\n",
      "swap rate: 0.12\n",
      "swap_0.12_0.csv\n",
      "swap rate: 0.13\n",
      "swap_0.13_0.csv\n",
      "swap rate: 0.14\n",
      "swap_0.14_0.csv\n",
      "swap rate: 0.15000000000000002\n",
      "swap_0.15000000000000002_0.csv\n",
      "swap rate: 0.16\n",
      "swap_0.16_0.csv\n",
      "swap rate: 0.17\n",
      "swap_0.17_0.csv\n",
      "swap rate: 0.18000000000000002\n",
      "swap_0.18000000000000002_0.csv\n",
      "swap rate: 0.19\n",
      "swap_0.19_0.csv\n",
      "swap rate: 0.2\n",
      "swap_0.2_0.csv\n",
      "swap rate: 0.21000000000000002\n",
      "swap_0.21000000000000002_0.csv\n",
      "swap rate: 0.22\n",
      "swap_0.22_0.csv\n",
      "swap rate: 0.23\n",
      "swap_0.23_0.csv\n",
      "swap rate: 0.24000000000000002\n",
      "swap_0.24000000000000002_0.csv\n",
      "swap rate: 0.25\n",
      "swap_0.25_0.csv\n",
      "swap rate: 0.26\n",
      "swap_0.26_0.csv\n",
      "swap rate: 0.27\n",
      "swap_0.27_0.csv\n",
      "swap rate: 0.28\n",
      "swap_0.28_0.csv\n",
      "swap rate: 0.29000000000000004\n",
      "swap_0.29000000000000004_0.csv\n",
      "swap rate: 0.3\n",
      "swap_0.3_0.csv\n",
      "swap rate: 0.31\n",
      "swap_0.31_0.csv\n",
      "swap rate: 0.32\n",
      "swap_0.32_0.csv\n",
      "swap rate: 0.33\n",
      "swap_0.33_0.csv\n",
      "swap rate: 0.34\n",
      "swap_0.34_0.csv\n",
      "swap rate: 0.35000000000000003\n",
      "swap_0.35000000000000003_0.csv\n",
      "swap rate: 0.36000000000000004\n",
      "swap_0.36000000000000004_0.csv\n",
      "swap rate: 0.37\n",
      "swap_0.37_0.csv\n",
      "swap rate: 0.38\n",
      "swap_0.38_0.csv\n",
      "swap rate: 0.39\n",
      "swap_0.39_0.csv\n",
      "swap rate: 0.4\n",
      "swap_0.4_0.csv\n",
      "swap rate: 0.41000000000000003\n",
      "swap_0.41000000000000003_0.csv\n",
      "swap rate: 0.42000000000000004\n",
      "swap_0.42000000000000004_0.csv\n",
      "swap rate: 0.43\n",
      "swap_0.43_0.csv\n",
      "swap rate: 0.44\n",
      "swap_0.44_0.csv\n",
      "swap rate: 0.45\n",
      "swap_0.45_0.csv\n",
      "swap rate: 0.46\n",
      "swap_0.46_0.csv\n",
      "swap rate: 0.47000000000000003\n",
      "swap_0.47000000000000003_0.csv\n",
      "swap rate: 0.48000000000000004\n",
      "swap_0.48000000000000004_0.csv\n",
      "swap rate: 0.49\n",
      "swap_0.49_0.csv\n",
      "swap rate: 0.5\n",
      "swap_0.5_0.csv\n",
      "swap rate: 0.51\n",
      "swap_0.51_0.csv\n",
      "swap rate: 0.52\n",
      "swap_0.52_0.csv\n",
      "swap rate: 0.53\n",
      "swap_0.53_0.csv\n",
      "swap rate: 0.54\n",
      "swap_0.54_0.csv\n",
      "swap rate: 0.55\n",
      "swap_0.55_0.csv\n",
      "swap rate: 0.56\n",
      "swap_0.56_0.csv\n",
      "swap rate: 0.5700000000000001\n",
      "swap_0.5700000000000001_0.csv\n",
      "swap rate: 0.5800000000000001\n",
      "swap_0.5800000000000001_0.csv\n",
      "swap rate: 0.59\n",
      "swap_0.59_0.csv\n",
      "swap rate: 0.6\n",
      "swap_0.6_0.csv\n",
      "swap rate: 0.61\n",
      "swap_0.61_0.csv\n",
      "swap rate: 0.62\n",
      "swap_0.62_0.csv\n",
      "swap rate: 0.63\n",
      "swap_0.63_0.csv\n",
      "swap rate: 0.64\n",
      "swap_0.64_0.csv\n",
      "swap rate: 0.65\n",
      "swap_0.65_0.csv\n",
      "swap rate: 0.66\n",
      "swap_0.66_0.csv\n",
      "swap rate: 0.67\n",
      "swap_0.67_0.csv\n",
      "swap rate: 0.68\n",
      "swap_0.68_0.csv\n",
      "swap rate: 0.6900000000000001\n",
      "swap_0.6900000000000001_0.csv\n",
      "swap rate: 0.7000000000000001\n",
      "swap_0.7000000000000001_0.csv\n",
      "swap rate: 0.7100000000000001\n",
      "swap_0.7100000000000001_0.csv\n",
      "swap rate: 0.72\n",
      "swap_0.72_0.csv\n",
      "swap rate: 0.73\n",
      "swap_0.73_0.csv\n",
      "swap rate: 0.74\n",
      "swap_0.74_0.csv\n",
      "swap rate: 0.75\n",
      "swap_0.75_0.csv\n",
      "swap rate: 0.76\n",
      "swap_0.76_0.csv\n",
      "swap rate: 0.77\n",
      "swap_0.77_0.csv\n",
      "swap rate: 0.78\n",
      "swap_0.78_0.csv\n",
      "swap rate: 0.79\n",
      "swap_0.79_0.csv\n",
      "swap rate: 0.8\n",
      "swap_0.8_0.csv\n",
      "swap rate: 0.81\n",
      "swap_0.81_0.csv\n",
      "swap rate: 0.8200000000000001\n",
      "swap_0.8200000000000001_0.csv\n",
      "swap rate: 0.8300000000000001\n",
      "swap_0.8300000000000001_0.csv\n",
      "swap rate: 0.8400000000000001\n",
      "swap_0.8400000000000001_0.csv\n",
      "swap rate: 0.85\n",
      "swap_0.85_0.csv\n",
      "swap rate: 0.86\n",
      "swap_0.86_0.csv\n",
      "swap rate: 0.87\n",
      "swap_0.87_0.csv\n",
      "swap rate: 0.88\n",
      "swap_0.88_0.csv\n",
      "swap rate: 0.89\n",
      "swap_0.89_0.csv\n",
      "swap rate: 0.9\n",
      "swap_0.9_0.csv\n",
      "swap rate: 0.91\n",
      "swap_0.91_0.csv\n",
      "swap rate: 0.92\n",
      "swap_0.92_0.csv\n",
      "swap rate: 0.93\n",
      "swap_0.93_0.csv\n",
      "swap rate: 0.9400000000000001\n",
      "swap_0.9400000000000001_0.csv\n",
      "swap rate: 0.9500000000000001\n",
      "swap_0.9500000000000001_0.csv\n",
      "swap rate: 0.9600000000000001\n",
      "swap_0.9600000000000001_0.csv\n",
      "swap rate: 0.97\n",
      "swap_0.97_0.csv\n",
      "swap rate: 0.98\n",
      "swap_0.98_0.csv\n",
      "swap rate: 0.99\n",
      "swap_0.99_0.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "races = ['White alone',\n",
    "         'Black or African American alone',\n",
    "         'American Indian and Alaska Native alone',\n",
    "         'Asian alone',\n",
    "         'Native Hawaiian and Other Pacific Islander alone',\n",
    "         'Some Other Race alone',\n",
    "         'White; Black or African American',\n",
    "         'White; American Indian and Alaska Native',\n",
    "         'White; Asian',\n",
    "         'White; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Some Other Race',\n",
    "         'Black or African American; American Indian and Alaska Native',\n",
    "         'Black or African American; Asian',\n",
    "         'Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "         'Black or African American; Some Other Race',\n",
    "         'American Indian and Alaska Native; Asian',\n",
    "         'American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "         'American Indian and Alaska Native; Some Other Race',\n",
    "         'Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'Asian; Some Other Race',\n",
    "         'Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Black or African American; American Indian and Alaska Native',\n",
    "         'White; Black or African American; Asian',\n",
    "         'White; Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Black or African American; Some Other Race',\n",
    "         'White; American Indian and Alaska Native; Asian',\n",
    "         'White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; American Indian and Alaska Native; Some Other Race',\n",
    "         'White; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Asian; Some Other Race',\n",
    "         'White; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'Black or African American; American Indian and Alaska Native; Asian',\n",
    "         'Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "         'Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "         'Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'Black or African American; Asian; Some Other Race',\n",
    "         'Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'American Indian and Alaska Native; Asian; Some Other Race',\n",
    "         'American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Asian',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "         'White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Black or African American; Asian; Some Other Race',\n",
    "         'White; Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "         'White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "         'Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race']\n",
    "\n",
    "races2 = ['Total!!Population of one race!!White alone',\n",
    "          'Total!!Population of one race!!Black or African American alone',\n",
    "          'Total!!Population of one race!!American Indian and Alaska Native alone',\n",
    "          'Total!!Population of one race!!Asian alone',\n",
    "          'Total!!Population of one race!!Native Hawaiian and Other Pacific Islander alone',\n",
    "          'Total!!Population of one race!!Some Other Race alone',\n",
    "          'Total!!Two or More Races!!Population of two races!!White; Black or African American',\n",
    "          'Total!!Two or More Races!!Population of two races!!White; American Indian and Alaska Native',\n",
    "          'Total!!Two or More Races!!Population of two races!!White; Asian',\n",
    "          'Total!!Two or More Races!!Population of two races!!White; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of two races!!White; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of two races!!Black or African American; American Indian and Alaska Native',\n",
    "          'Total!!Two or More Races!!Population of two races!!Black or African American; Asian',\n",
    "          'Total!!Two or More Races!!Population of two races!!Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of two races!!Black or African American; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Asian',\n",
    "          'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of two races!!Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of two races!!Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of two races!!Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Black or African American; American Indian and Alaska Native',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Black or African American; Asian',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Black or African American; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Asian',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Asian',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Asian',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of five races!!White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of five races!!White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of five races!!Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of six races!!White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race']\n",
    "\n",
    "#re_id function re-written using pandas operations for massive performance improvements\n",
    "#note this assumes that index keys are the same between the index vals\n",
    "#validated aganist original privacy functions for accuracy\n",
    "def re_id(D, P, M, theta_d, theta_p, indices, swapped):\n",
    "    # D: de-identified dataset\n",
    "    # P: public dataset\n",
    "    # M: set of attributes shared between D and P\n",
    "    # alpha: attribute set size limit in M. we only examine attribute tuples where at most alpha entries in M have that tuple\n",
    "    # we remove alpha here and look at sets of all sizes, since runtime is not an issue\n",
    "    # theta_d: attribute set size limit in D. we only examine attribute tuples with at most theta_d matching entries in D\n",
    "    # theta_p: attribute set size limit in P.\n",
    "    # theta_d, and theta_p are used to limit compute costs. we can set them to |D|\n",
    "    # indices: list of index triples. a triple (i,j, k) indicates that attribute k in M corresponds to attribute i in D and attribute j in P\n",
    "    # paper used alpha=7, theta_d = 10, theta_p = 5\n",
    "    # D, P, and M must have attributes in the same order. matching attributes must come first in D and P\n",
    "    # swapped: indicates whether D was anonymized via swapping\n",
    "    num_vulnerable = 0\n",
    "    num_identified = 0\n",
    "    V = [] #list of vulnerable entries\n",
    "    for row in M:\n",
    "        #true case\n",
    "        att_val_combos = D.loc[(D['age'] == row[0]) &\n",
    "                               (D['race'] == row[1]) &\n",
    "                               (D['household_size'] == row[2])]\n",
    "        if len(att_val_combos) <= theta_d and len(att_val_combos) > 0:\n",
    "            #false case - not using indices in theory fine\n",
    "            p_matches = P.loc[(P['age'] == row[0]) &\n",
    "                                   (P['race'] == row[1]) &\n",
    "                                   (P['household_size'] == row[2])]\n",
    "            if len(p_matches) <= theta_p and len(p_matches) > 0:\n",
    "                if swapped:\n",
    "                    count = att_val_combos[att_val_combos['SwapVal'] == False].shape[0]\n",
    "                    num_vulnerable += count\n",
    "                    if (len(p_matches) == 1):\n",
    "                        num_identified += count\n",
    "                else:\n",
    "                    num_vulnerable += len(p_matches)\n",
    "                    if (len(p_matches) == 1):\n",
    "                        num_identified += len(p_matches)\n",
    "                V.append([row, att_val_combos, p_matches])\n",
    "    return V, num_vulnerable, num_identified\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "# CREATE PUBLIC DATA\n",
    "# input: csv containing block data\n",
    "# output: dataframe containing public dataset with age and sex\n",
    "##########################################################################################################################\n",
    "\n",
    "def create_public_data_age_sex(path, races):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    for col_name, data in df.items():\n",
    "        if col_name == 'race':\n",
    "            oof = 0\n",
    "            for d in data:\n",
    "                if d in races:\n",
    "                    df.at[oof, 'race'] = races.index(d)\n",
    "                oof += 1\n",
    "            oof = 0\n",
    "            for d in data:\n",
    "                if d in races2:\n",
    "                    df.at[oof, 'race'] = races2.index(d)\n",
    "                oof += 1\n",
    "        elif col_name == 'age':\n",
    "            oof = 0\n",
    "            for d in data:\n",
    "                if d<18:\n",
    "                    df.at[oof, 'age'] = 0\n",
    "                else:\n",
    "                    df.at[oof, 'age'] = 1\n",
    "                oof+=1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def measure_privacy(county, races):\n",
    "    # prints privacy value for each swap rate for the given county\n",
    "    swap_rates = np.arange(.01, 1.0, .01, float).tolist()\n",
    "\n",
    "    indices = [['age', 'age', 0], ['race', 'race', 1], ['household_size', 'household_size', 2]]\n",
    "    # Set up datasets\n",
    "\n",
    "    # Common attributes\n",
    "    M = []\n",
    "    for age in [0,1]:\n",
    "        for race in range(0, len(races)):\n",
    "            for size in [1, 2, 3, 4]:\n",
    "                M.append([age, race, size])\n",
    "\n",
    "    # Original dataset\n",
    "    P = create_public_data_age_sex('../homemade_data/' + county + '.csv', races)\n",
    "\n",
    "    # create arrays where we will record privacy values\n",
    "    stats1 = []\n",
    "    stats2 = []\n",
    "\n",
    "    # evaluate privacy for datasets of each swap rate\n",
    "    for rate in swap_rates:\n",
    "        #try for all of a given swap rate:        \n",
    "        print('swap rate: ' + str(rate))\n",
    "        srvals1 = []\n",
    "        srvals2 = []\n",
    "        for filename in os.listdir('../swapping/swap_runs/'+county+'/similar/'):\n",
    "            directory = '../swapping/swap_runs/'+county+'/similar/'\n",
    "            if filename.endswith(\".csv\") and (str(rate)+\"_\" in filename):  \n",
    "                D1 = create_public_data_age_sex(directory+filename, races)\n",
    "                V1, num_vulnerable1, num_identified1 = re_id(D1, P, M, 3, 3, indices, True)\n",
    "                srvals1.append([rate, num_vulnerable1, num_identified1])\n",
    "                \n",
    "        srvals1_id_total = [0,0]\n",
    "        srvals1_vul_total = [0,0]\n",
    "        for s in srvals1:\n",
    "            srvals1_id_total[0] = srvals1_id_total[0] + s[2]\n",
    "            srvals1_id_total[1] = srvals1_id_total[1] + 1\n",
    "            srvals1_vul_total[0] = srvals1_vul_total[0] + s[1]\n",
    "            srvals1_vul_total[1] = srvals1_vul_total[1] + 1\n",
    "        id1 = srvals1_id_total[0]/srvals1_id_total[1]\n",
    "        vul1 = srvals1_vul_total[0]/srvals1_vul_total[1]\n",
    "                \n",
    "        for filename in os.listdir('../swapping/swap_runs/'+county+'/random/'):  \n",
    "            directory = '../swapping/swap_runs/'+county+'/random/'\n",
    "            if filename.endswith(\".csv\") and (rate in filename):  \n",
    "                print(rate, filename)\n",
    "                D2 = create_public_data_age_sex(directory+filename, races)\n",
    "                V2, num_vulnerable2, num_identified2 = re_id(D2, P, M, 3, 3, indices, True)\n",
    "                srvals2.append([rate, num_vulnerable2, num_identified2])\n",
    "               \n",
    "        srvals2_id_total = [0,0]  \n",
    "        srvals2_vul_total = [0,0] \n",
    "        for s in srvals2:\n",
    "            srvals2_id_total[0] = srvals2_id_total[0] + s[2]\n",
    "            srvals2_id_total[1] = srvals2_id_total[1] + 1\n",
    "            srvals2_vul_total[0] = srvals2_vul_total[0] + s[1]\n",
    "            srvals2_vul_total[1] = srvals2_vul_total[1] + 1\n",
    "        id2 = srvals2_id_total[0]/srvals2_id_total[1]\n",
    "        vul2 = srvals2_vul_total[0]/srvals2_vul_total[1]\n",
    "                \n",
    "        stats1.append([rate,vul1,id1])\n",
    "        stats2.append([rate,vul2,id2])\n",
    "        print([rate,vul1,id1])\n",
    "        print(([rate,vul2,id2]))\n",
    "    return stats1, stats2\n",
    "\n",
    "counties = ['alameda']\n",
    "lines = []\n",
    "\n",
    "for county in counties:\n",
    "    lines.append(f'\\n\\n{county}\\n----------------------------------------------')\n",
    "    print('now analyzing ', county)\n",
    "    wash1, wash2 = measure_privacy(county, races)\n",
    "    print(wash1, wash2)\n",
    "    print(county, ' similar:')\n",
    "    print(wash1)\n",
    "    lines.append(f'{county} Similar:\\n{wash1}')\n",
    "    print(county, ' random:')\n",
    "    print(wash2)\n",
    "    lines.append(f'{county} Random:\\n{wash2}')\n",
    "\n",
    "with open('FINAL_PRIVACY_RESULTS.txt', \"a\") as f:\n",
    "    for l in lines:\n",
    "        f.write(l + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulates a linkage attack in which the attacker knows (voting age Y/N) x (race) x (household size) \n",
    "# and attempts to learn (hisp Y/N). \n",
    "# The attacker first finds unique and nearly unique (\"nearly\" is specified by vul_param) entries in the public dataset.\n",
    "# \n",
    "# Returns the number of identified entries and the number of vulnerable entries with the correct hisp value.\n",
    "#\n",
    "#INPUT: \n",
    "# P: public dataset\n",
    "# noisy_queries: list containing all possible combinations/iterations of data and a secondary value\n",
    "#                       [[age, hisp, race, size], noisy_count_value]\n",
    "# vul_param: integer specifying what is considered a vulnerable entry. E.g., if vul_param = 3, a vulnerable entry is one\n",
    "#                 with at most 3 matching entries in the public database and 3 matching entries in the DP data\n",
    "#---------------------------------------------------------------------\n",
    "#OUTPUT:\n",
    "#\n",
    "def measure_privacy_dp(P, noisy_queries, vul_param):\n",
    "    # create a dictionary corresponding to noisy_queries for faster lookup\n",
    "    d = {}\n",
    "    for query in noisy_queries:\n",
    "        d[str(query[0])] = query[1]\n",
    "    print(d[str([1, 0, 3, 1])])\n",
    "    \n",
    "    identified_entries = []\n",
    "    vulnerable_entries = []\n",
    "    for age in [0, 1]:\n",
    "        for race in range(63):\n",
    "            for size in [1, 2, 3, 4]:\n",
    "                \n",
    "                # for current combo of (age, race, size), find the matching queries\n",
    "                count = 0\n",
    "                hisp_count = 0\n",
    "                nonhisp_count = 0\n",
    "                \n",
    "                nonhisp_query = d[str([age, 0, race, size])]\n",
    "                hisp_query = d[str([age, 1, race, size])]\n",
    "\n",
    "                \n",
    "                count += max(0, nonhisp_query)\n",
    "                count += max(0, hisp_query)\n",
    "\n",
    "                nonhisp_count += max(0, nonhisp_query)\n",
    "                hisp_count += max(0, hisp_query)\n",
    "                \n",
    "#                 if age == 1 and race == 3 and size == 1:\n",
    "#                     print(nonhisp_count)\n",
    "#                     print(hisp_count)\n",
    "#                     print(count)\n",
    "                \n",
    "                # if entry is unique, guess whether it is hispanic or nonhispanic based on which of those counts is positive\n",
    "                if count == 1:\n",
    "                    hisp_val = 0\n",
    "                    if hisp_count >= 1:\n",
    "                        hisp_val = 1\n",
    "                    identified_entries.append([[age, race, size], hisp_val])\n",
    "                \n",
    "                # if entry is vulnerable, guess whether it is hispanic or nonhispanic by flipping a biased coin based on the \n",
    "                # values of nonhisp_query and hisp_query\n",
    "                if count >= 1 and count <= vul_param:\n",
    "                    hisp_val = np.random.binomial(1, hisp_count/count)\n",
    "                    vulnerable_entries.append([[age, race, size], hisp_val])\n",
    "    return identified_entries, vulnerable_entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "([], [[[1, 3, 1], 1]])\n"
     ]
    }
   ],
   "source": [
    "noisy_queries = []\n",
    "for age in [0, 1]:\n",
    "    for race in range(63):\n",
    "        for size in [1, 2, 3, 4]:\n",
    "            if age == 1 and race == 3:\n",
    "                noisy_queries.append([[age, 0, race, 1], 1])\n",
    "                noisy_queries.append([[age, 1, race, 1], 2])\n",
    "                \n",
    "                noisy_queries.append([[age, 0, race, 2], 0])\n",
    "                noisy_queries.append([[age, 0, race, 3], 0])\n",
    "                noisy_queries.append([[age, 0, race, 4], 0])\n",
    "                for i in [2, 3, 4]:\n",
    "                    noisy_queries.append([[age, 1, race, i], 0])\n",
    "            else:\n",
    "                noisy_queries.append([[age, 0, race, size], 0])\n",
    "                noisy_queries.append([[age, 1, race, size], 0])\n",
    "# print(noisy_queries)\n",
    "# print(noisy_queries)\n",
    "print(measure_privacy_dp(noisy_queries, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
