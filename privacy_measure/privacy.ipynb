{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now analyzing  Washington\n",
      "swap rate: 0.9600000000000001\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../swapping/swap_runs/Washington/similar/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-0d802545c8b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\n\\n{county}\\n----------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'now analyzing '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mwash1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwash2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure_privacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwash1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwash2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' similar:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-0d802545c8b1>\u001b[0m in \u001b[0;36mmeasure_privacy\u001b[0;34m(county, races)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0msrvals1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0msrvals2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../swapping/swap_runs/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcounty\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/similar/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../swapping/swap_runs/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcounty\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/similar/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../swapping/swap_runs/Washington/similar/'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "races = ['White alone',\n",
    "         'Black or African American alone',\n",
    "         'American Indian and Alaska Native alone',\n",
    "         'Asian alone',\n",
    "         'Native Hawaiian and Other Pacific Islander alone',\n",
    "         'Some Other Race alone',\n",
    "         'White; Black or African American',\n",
    "         'White; American Indian and Alaska Native',\n",
    "         'White; Asian',\n",
    "         'White; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Some Other Race',\n",
    "         'Black or African American; American Indian and Alaska Native',\n",
    "         'Black or African American; Asian',\n",
    "         'Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "         'Black or African American; Some Other Race',\n",
    "         'American Indian and Alaska Native; Asian',\n",
    "         'American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "         'American Indian and Alaska Native; Some Other Race',\n",
    "         'Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'Asian; Some Other Race',\n",
    "         'Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Black or African American; American Indian and Alaska Native',\n",
    "         'White; Black or African American; Asian',\n",
    "         'White; Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Black or African American; Some Other Race',\n",
    "         'White; American Indian and Alaska Native; Asian',\n",
    "         'White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; American Indian and Alaska Native; Some Other Race',\n",
    "         'White; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Asian; Some Other Race',\n",
    "         'White; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'Black or African American; American Indian and Alaska Native; Asian',\n",
    "         'Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "         'Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "         'Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'Black or African American; Asian; Some Other Race',\n",
    "         'Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'American Indian and Alaska Native; Asian; Some Other Race',\n",
    "         'American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Asian',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "         'White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Black or African American; Asian; Some Other Race',\n",
    "         'White; Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "         'White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "         'Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "         'White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race']\n",
    "\n",
    "races2 = ['Total!!Population of one race!!White alone',\n",
    "          'Total!!Population of one race!!Black or African American alone',\n",
    "          'Total!!Population of one race!!American Indian and Alaska Native alone',\n",
    "          'Total!!Population of one race!!Asian alone',\n",
    "          'Total!!Population of one race!!Native Hawaiian and Other Pacific Islander alone',\n",
    "          'Total!!Population of one race!!Some Other Race alone',\n",
    "          'Total!!Two or More Races!!Population of two races!!White; Black or African American',\n",
    "          'Total!!Two or More Races!!Population of two races!!White; American Indian and Alaska Native',\n",
    "          'Total!!Two or More Races!!Population of two races!!White; Asian',\n",
    "          'Total!!Two or More Races!!Population of two races!!White; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of two races!!White; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of two races!!Black or African American; American Indian and Alaska Native',\n",
    "          'Total!!Two or More Races!!Population of two races!!Black or African American; Asian',\n",
    "          'Total!!Two or More Races!!Population of two races!!Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of two races!!Black or African American; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Asian',\n",
    "          'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of two races!!American Indian and Alaska Native; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of two races!!Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of two races!!Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of two races!!Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Black or African American; American Indian and Alaska Native',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Black or African American; Asian',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Black or African American; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Black or African American; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Asian',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; American Indian and Alaska Native; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!White; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Asian',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of three races!!Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Asian',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; American Indian and Alaska Native; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Black or African American; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!White; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of four races!!American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander',\n",
    "          'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Asian; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of five races!!White; Black or African American; American Indian and Alaska Native; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of five races!!White; Black or African American; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of five races!!White; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of five races!!Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race',\n",
    "          'Total!!Two or More Races!!Population of six races!!White; Black or African American; American Indian and Alaska Native; Asian; Native Hawaiian and Other Pacific Islander; Some Other Race']\n",
    "\n",
    "#re_id function re-written using pandas operations for massive performance improvements\n",
    "#note this assumes that index keys are the same between the index vals\n",
    "#validated aganist original privacy functions for accuracy\n",
    "def re_id(D, P, M, theta_d, theta_p, indices, swapped):\n",
    "    # D: de-identified dataset\n",
    "    # P: public dataset\n",
    "    # M: set of attributes shared between D and P\n",
    "    # alpha: attribute set size limit in M. we only examine attribute tuples where at most alpha entries in M have that tuple\n",
    "    # we remove alpha here and look at sets of all sizes, since runtime is not an issue\n",
    "    # theta_d: attribute set size limit in D. we only examine attribute tuples with at most theta_d matching entries in D\n",
    "    # theta_p: attribute set size limit in P.\n",
    "    # theta_d, and theta_p are used to limit compute costs. we can set them to |D|\n",
    "    # indices: list of index triples. a triple (i,j, k) indicates that attribute k in M corresponds to attribute i in D and attribute j in P\n",
    "    # paper used alpha=7, theta_d = 10, theta_p = 5\n",
    "    # D, P, and M must have attributes in the same order. matching attributes must come first in D and P\n",
    "    # swapped: indicates whether D was anonymized via swapping\n",
    "    num_vulnerable = 0\n",
    "    num_identified = 0\n",
    "    V = [] #list of vulnerable entries\n",
    "    for row in M:\n",
    "        #true case\n",
    "        att_val_combos = D.loc[(D['age'] == row[0]) &\n",
    "                               (D['race'] == row[1]) &\n",
    "                               (D['household_size'] == row[2])]\n",
    "        if len(att_val_combos) <= theta_d and len(att_val_combos) > 0:\n",
    "            #false case - not using indices in theory fine\n",
    "            p_matches = P.loc[(P['age'] == row[0]) &\n",
    "                                   (P['race'] == row[1]) &\n",
    "                                   (P['household_size'] == row[2])]\n",
    "            if len(p_matches) <= theta_p and len(p_matches) > 0:\n",
    "                if swapped:\n",
    "                    count = att_val_combos[att_val_combos['SwapVal'] == False].shape[0]\n",
    "                    num_vulnerable += count\n",
    "                    if (len(p_matches) == 1):\n",
    "                        num_identified += count\n",
    "                else:\n",
    "                    num_vulnerable += len(p_matches)\n",
    "                    if (len(p_matches) == 1):\n",
    "                        num_identified += len(p_matches)\n",
    "                V.append([row, att_val_combos, p_matches])\n",
    "    return V, num_vulnerable, num_identified\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "# CREATE PUBLIC DATA\n",
    "# input: csv containing block data\n",
    "# output: dataframe containing public dataset with age and sex\n",
    "##########################################################################################################################\n",
    "\n",
    "def create_public_data_age_sex(path, races):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    for col_name, data in df.items():\n",
    "        if col_name == 'race':\n",
    "            oof = 0\n",
    "            for d in data:\n",
    "                if d in races:\n",
    "                    df.at[oof, 'race'] = races.index(d)\n",
    "                oof += 1\n",
    "            oof = 0\n",
    "            for d in data:\n",
    "                if d in races2:\n",
    "                    df.at[oof, 'race'] = races2.index(d)\n",
    "                oof += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def measure_privacy(county, races):\n",
    "    # prints privacy value for each swap rate for the given county\n",
    "    swap_rates = ['0.9600000000000001']\n",
    "\n",
    "    indices = [['age', 'age', 0], ['race', 'race', 1], ['household_size', 'household_size', 2]]\n",
    "    # Set up datasets\n",
    "\n",
    "    # Common attributes\n",
    "    M = []\n",
    "    for age in range(90):\n",
    "        for race in range(0, len(races)):\n",
    "            for size in [1, 2, 3, 4]:\n",
    "                M.append([age, race, size])\n",
    "\n",
    "    # Original dataset\n",
    "    P = create_public_data_age_sex('../homemade_data/' + county + '.csv', races)\n",
    "\n",
    "    # create arrays where we will record privacy values\n",
    "    stats1 = []\n",
    "    stats2 = []\n",
    "\n",
    "    # evaluate privacy for datasets of each swap rate\n",
    "    for rate in swap_rates:\n",
    "        #try for all of a given swap rate:        \n",
    "        print('swap rate: ' + str(rate))\n",
    "        srvals1 = []\n",
    "        srvals2 = []\n",
    "        for filename in os.listdir('../swapping/swap_runs/'+county+'/similar/'):\n",
    "            directory = '../swapping/swap_runs/'+county+'/similar/'\n",
    "            if filename.endswith(\".csv\") and (rate in filename):  \n",
    "                print(rate, filename)\n",
    "                D1 = create_public_data_age_sex(directory+filename, races)\n",
    "                V1, num_vulnerable1, num_identified1 = re_id(D1, P, M, 3, 3, indices, True)\n",
    "                srvals1.append([rate, num_vulnerable1, num_identified1])\n",
    "                \n",
    "        srvals1_id_total = [0,0]\n",
    "        srvals1_vul_total = [0,0]\n",
    "        for s in srvals1:\n",
    "            srvals1_id_total[0] = srvals1_id_total[0] + s[2]\n",
    "            srvals1_id_total[1] = srvals1_id_total[1] + 1\n",
    "            srvals1_vul_total[0] = srvals1_vul_total[0] + s[1]\n",
    "            srvals1_vul_total[1] = srvals1_vul_total[1] + 1\n",
    "        id1 = srvals1_id_total[0]/srvals1_id_total[1]\n",
    "        vul1 = srvals1_vul_total[0]/srvals1_vul_total[1]\n",
    "                \n",
    "        for filename in os.listdir('../swapping/swap_runs/'+county+'/random/'):  \n",
    "            directory = '../swapping/swap_runs/'+county+'/random/'\n",
    "            if filename.endswith(\".csv\") and (rate in filename):  \n",
    "                print(rate, filename)\n",
    "                D2 = create_public_data_age_sex(directory+filename, races)\n",
    "                V2, num_vulnerable2, num_identified2 = re_id(D2, P, M, 3, 3, indices, True)\n",
    "                srvals2.append([rate, num_vulnerable2, num_identified2])\n",
    "               \n",
    "        srvals2_id_total = [0,0]  \n",
    "        srvals2_vul_total = [0,0] \n",
    "        for s in srvals2:\n",
    "            srvals2_id_total[0] = srvals2_id_total[0] + s[2]\n",
    "            srvals2_id_total[1] = srvals2_id_total[1] + 1\n",
    "            srvals2_vul_total[0] = srvals2_vul_total[0] + s[1]\n",
    "            srvals2_vul_total[1] = srvals2_vul_total[1] + 1\n",
    "        id2 = srvals2_id_total[0]/srvals2_id_total[1]\n",
    "        vul2 = srvals2_vul_total[0]/srvals2_vul_total[1]\n",
    "                \n",
    "        stats1.append([rate,vul1,id1])\n",
    "        stats2.append([rate,vul2,id2])\n",
    "        print([rate,vul1,id1])\n",
    "        print(([rate,vul2,id2]))\n",
    "    return stats1, stats2\n",
    "\n",
    "counties = ['Washington']\n",
    "lines = []\n",
    "\n",
    "for county in counties:\n",
    "    lines.append(f'\\n\\n{county}\\n----------------------------------------------')\n",
    "    print('now analyzing ', county)\n",
    "    wash1, wash2 = measure_privacy(county, races)\n",
    "    print(wash1, wash2)\n",
    "    print(county, ' similar:')\n",
    "    print(wash1)\n",
    "    lines.append(f'{county} Similar:\\n{wash1}')\n",
    "    print(county, ' random:')\n",
    "    print(wash2)\n",
    "    lines.append(f'{county} Random:\\n{wash2}')\n",
    "\n",
    "with open('FINAL_PRIVACY_RESULTS.txt', \"a\") as f:\n",
    "    for l in lines:\n",
    "        f.write(l + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulates a linkage attack in which the attacker knows (voting age Y/N) x (race) x (household size) \n",
    "# and attempts to learn (hisp Y/N). \n",
    "# The attacker first finds unique and nearly unique (\"nearly\" is specified by vul_param) entries in the public dataset.\n",
    "# \n",
    "# Returns the number of identified entries and the number of vulnerable entries with the correct hisp value.\n",
    "#\n",
    "#INPUT: \n",
    "# P: public dataset\n",
    "# noisy_queries: list containing all possible combinations/iterations of data and a secondary value\n",
    "#                       [[age, hisp, race, size], noisy_count_value]\n",
    "# vul_param: integer specifying what is considered a vulnerable entry. E.g., if vul_param = 3, a vulnerable entry is one\n",
    "#                 with at most 3 matching entries in the public database and 3 matching entries in the DP data\n",
    "#---------------------------------------------------------------------\n",
    "#OUTPUT:\n",
    "#\n",
    "def measure_privacy_dp(P, noisy_queries, vul_param):\n",
    "    # create a dictionary corresponding to noisy_queries for faster lookup\n",
    "    d = {}\n",
    "    for query in noisy_queries:\n",
    "        d[str(query[0])] = query[1]\n",
    "    print(d[str([1, 0, 3, 1])])\n",
    "    \n",
    "    identified_entries = []\n",
    "    vulnerable_entries = []\n",
    "    for age in [0, 1]:\n",
    "        for race in range(63):\n",
    "            for size in [1, 2, 3, 4]:\n",
    "                \n",
    "                # for current combo of (age, race, size), find the matching queries\n",
    "                count = 0\n",
    "                hisp_count = 0\n",
    "                nonhisp_count = 0\n",
    "                \n",
    "                nonhisp_query = d[str([age, 0, race, size])]\n",
    "                hisp_query = d[str([age, 1, race, size])]\n",
    "\n",
    "                \n",
    "                count += max(0, nonhisp_query)\n",
    "                count += max(0, hisp_query)\n",
    "\n",
    "                nonhisp_count += max(0, nonhisp_query)\n",
    "                hisp_count += max(0, hisp_query)\n",
    "                \n",
    "#                 if age == 1 and race == 3 and size == 1:\n",
    "#                     print(nonhisp_count)\n",
    "#                     print(hisp_count)\n",
    "#                     print(count)\n",
    "                \n",
    "                # if entry is unique, guess whether it is hispanic or nonhispanic based on which of those counts is positive\n",
    "                if count == 1:\n",
    "                    hisp_val = 0\n",
    "                    if hisp_count >= 1:\n",
    "                        hisp_val = 1\n",
    "                    identified_entries.append([[age, race, size], hisp_val])\n",
    "                \n",
    "                # if entry is vulnerable, guess whether it is hispanic or nonhispanic by flipping a biased coin based on the \n",
    "                # values of nonhisp_query and hisp_query\n",
    "                if count >= 1 and count <= vul_param:\n",
    "                    hisp_val = np.random.binomial(1, hisp_count/count)\n",
    "                    vulnerable_entries.append([[age, race, size], hisp_val])\n",
    "    return identified_entries, vulnerable_entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "([], [[[1, 3, 1], 1]])\n"
     ]
    }
   ],
   "source": [
    "# noisy_queries = []\n",
    "# for age in [0, 1]:\n",
    "#     for race in range(63):\n",
    "#         for size in [1, 2, 3, 4]:\n",
    "#             if age == 1 and race == 3:\n",
    "#                 noisy_queries.append([[age, 0, race, 1], 1])\n",
    "#                 noisy_queries.append([[age, 1, race, 1], 2])\n",
    "                \n",
    "#                 noisy_queries.append([[age, 0, race, 2], 0])\n",
    "#                 noisy_queries.append([[age, 0, race, 3], 0])\n",
    "#                 noisy_queries.append([[age, 0, race, 4], 0])\n",
    "#                 for i in [2, 3, 4]:\n",
    "#                     noisy_queries.append([[age, 1, race, i], 0])\n",
    "#             else:\n",
    "#                 noisy_queries.append([[age, 0, race, size], 0])\n",
    "#                 noisy_queries.append([[age, 1, race, size], 0])\n",
    "# # print(noisy_queries)\n",
    "# # print(noisy_queries)\n",
    "# print(measure_privacy_dp(noisy_queries, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
